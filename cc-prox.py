#!/usr/bin/env python3
"""
Classical Chinese Document Processor (Qwen-VL + Kimi)
OCR with Qwen-VL, cleaning with Kimi
Outputs single consolidated markdown file with coherent text
"""

import sys
import os
import json
import re
import time
import tempfile
import base64
import argparse
from pathlib import Path
from datetime import datetime
from typing import List, Tuple, Optional, Dict, Any
from dataclasses import dataclass

# Third-party imports
import requests
from openai import OpenAI

# Optional dependencies
try:
    from pdf2image import convert_from_path, pdfinfo_from_path
    PDF_SUPPORT = True
except ImportError:
    PDF_SUPPORT = False
    print("âš ï¸  Warning: pdf2image not installed. PDF support disabled.")

try:
    import cv2
    import numpy as np
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    print("âš ï¸  Warning: opencv-python not installed. Image preprocessing disabled.")

# ============================================================================
# CONFIGURATION MANAGEMENT
# ============================================================================

@dataclass
class ProcessingConfig:
    """Centralized configuration for document processing"""
    # OCR Settings
    ocr_dpi: int = 200
    qwen_model: str = "qwen-vl-max"
    max_image_width: int = 1024

    # OCR Quality Thresholds
    expected_chars_normal: int = 500   # Flag if above this
    ocr_soft_cap: int = 800           # Warn if above this
    ocr_hard_cap: int = 1200          # Truncate if above this

    # Text Processing Settings
    max_text_length: int = 2400
    chunk_overlap: int = 400
    api_timeout: int = 300
    retry_attempts: int = 3
    retry_delay: int = 2

    # Model Costs
    model_costs: Dict[str, float] = None

    def __post_init__(self):
        if self.model_costs is None:
            self.model_costs = {
                "qwen-vl-plus": 0.004,
                "qwen-vl-max": 0.008,
            }

    def update_from_args(self, args):
        """Update config from command line arguments"""
        if hasattr(args, 'dpi'):
            self.ocr_dpi = args.dpi
        if hasattr(args, 'model'):
            self.qwen_model = args.model

# ============================================================================
# CLIENT MANAGEMENT
# ============================================================================

class APIClients:
    """Manages API clients for different services"""

    def __init__(self):
        if not self.validate_keys():
            raise Exception("API keys not configured")

        self.kimi = OpenAI(
            api_key=os.getenv("KIMI_API_KEY"),
            base_url="https://api.moonshot.cn/v1"
        )

        self.qwen = OpenAI(
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url="https://dashscope-intl.aliyuncs.com/compatible-mode/v1",
        )

    @staticmethod
    def validate_keys():
        """Validate that required API keys are set"""
        if not os.getenv("DASHSCOPE_API_KEY"):
            print("âŒ Error: DASHSCOPE_API_KEY environment variable not set")
            return False
        if not os.getenv("KIMI_API_KEY"):
            print("âŒ Error: KIMI_API_KEY environment variable not set")
            return False
        return True

# ============================================================================
# TEXT PROCESSING UTILITIES
# ============================================================================

class TextProcessor:
    """Handles text validation and cleaning operations"""

    @staticmethod
    def detect_ocr_loops_simple(text: str, sample_size: int = 200, max_repeats: int = 3) -> bool:
        """Quick check for obvious OCR repetition artifacts"""
        if len(text) < 1000:  # Only check longer texts
            return False

        # Sample a few spots for repetition
        check_length = min(len(text), 2000)
        for i in range(0, check_length - 100, sample_size):
            sample = text[i:i+100]
            if len(sample) < 100:
                continue
            if text.count(sample) > max_repeats:
                return True
        return False

    @staticmethod
    def remove_metadata_text(text: str) -> str:
        """Remove common institutional metadata from OCR text"""
        patterns = [
            r'å›½ç«‹å…¬æ–‡æ›¸é¤¨', r'National Archives of Japan', r'å…§é–£æ–‡åº«',
            r'ç•ªè™Ÿ\s*æ¼¢?\s*\d+', r'å†Šæ•¸\s*\d+', r'å·è™Ÿ\s*\d+',
            r'colorchecker', r'Kodak Gray Scale', r'x-rite',
            r'MSCCPPCC\d+', r'MSCCPPPE\d+', r'Kodak, 2007 TM: Kodak',
            r'ã€”.*?ã€•',
        ]

        for pattern in patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)

        # Clean up lines
        lines = text.split('\n')
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if line and not any(keyword in line for keyword in ['ç¶´å¿ƒéƒ¨', 'æ–‡å­—ç‚ºé–‹ä¸é®®æ˜']):
                cleaned_lines.append(line)

        return '\n'.join(cleaned_lines)

    @staticmethod
    def remove_explanatory_text(text: str) -> str:
        """Remove any explanatory notes added by AI"""
        patterns = [
            r'---\s*\n\*\*æ”¹å†™è¯´æ˜\*\*.*?(?=---|\Z)',
            r'ã€”æ”¹å†™è¯´æ˜ã€•.*?ã€”/æ”¹å†™è¯´æ˜ã€•',
            r'ã€”æ¶¦è‰²è¯´æ˜ã€•.*?ã€”/æ¶¦è‰²è¯´æ˜ã€•',
            r'\næ³¨ï¼š.*', r'\nè¯´æ˜ï¼š.*',
        ]

        for pattern in patterns:
            text = re.sub(pattern, '', text, flags=re.DOTALL)

        # Remove standalone explanatory lines
        lines = text.split('\n')
        cleaned_lines = []
        skip_next = False

        for line in lines:
            line = line.strip()
            if any(keyword in line for keyword in ['æ”¹å†™è¯´æ˜', 'æ¶¦è‰²è¯´æ˜', 'è°ƒæ•´è¯´æ˜']):
                skip_next = True
                continue
            if skip_next and line.startswith('---'):
                skip_next = False
                continue
            if not skip_next and line:
                cleaned_lines.append(line)

        return '\n'.join(cleaned_lines)

# ============================================================================
# IMAGE PROCESSING
# ============================================================================

class ImageProcessor:
    """Handles image preprocessing operations"""

    @staticmethod
    def preprocess_image(image_path: str, config: ProcessingConfig) -> str:
        """Improve OCR quality by preprocessing image"""
        if not CV2_AVAILABLE:
            return image_path

        try:
            img = cv2.imread(image_path)
            if img is None:
                return image_path

            # Convert to grayscale
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

            # Resize if too large
            h, w = gray.shape
            if w > config.max_image_width:
                ratio = config.max_image_width / w
                new_h = int(h * ratio)
                gray = cv2.resize(gray, (config.max_image_width, new_h), interpolation=cv2.INTER_AREA)
                print(f"    Resized image from {w}x{h} to {config.max_image_width}x{new_h}")

            # Denoise and binarize
            denoised = cv2.fastNlMeansDenoising(gray, h=10, templateWindowSize=7, searchWindowSize=21)
            _, binary = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

            # Save preprocessed image
            temp_file = tempfile.NamedTemporaryFile(suffix='_preprocessed.png', delete=False)
            cv2.imwrite(temp_file.name, binary)
            return temp_file.name

        except Exception as e:
            print(f"Preprocessing error: {e}")
            return image_path

    @staticmethod
    def truncate_at_loop(text: str, max_length: int = 500) -> str:
        """Truncate text at first major repetition pattern"""
        for i in range(0, min(len(text), max_length), 50):
            sample = text[i:i+100]
            if len(sample) < 100:
                continue
            # If this 100-char chunk appears again, cut before second occurrence
            second_occurrence = text.find(sample, i+100)
            if second_occurrence != -1:
                print(f"    âœ‚ï¸ Cutting at char {second_occurrence} (loop detected)")
                return text[:second_occurrence]
        return text[:max_length]

# ============================================================================
# ZOTERO INTEGRATION
# ============================================================================

class ZoteroExporter:
    """Handles exporting to Zotero"""
    
    @staticmethod
    def get_collection_key(collection_name: str) -> Optional[str]:
        """Look up Zotero collection key by name"""
        api_key = os.getenv("ZOTERO_API_KEY")
        user_id = os.getenv("ZOTERO_USER_ID")
        
        if not api_key or not user_id:
            return None
        
        try:
            headers = {"Zotero-API-Key": api_key}
            response = requests.get(
                f"https://api.zotero.org/users/{user_id}/collections",
                headers=headers
            )
            
            if response.status_code != 200:
                return None
            
            collections = response.json()
            for collection in collections:
                if collection['data']['name'] == collection_name:
                    return collection['data']['key']
            
            print(f"âš ï¸  Collection '{collection_name}' not found in Zotero library")
            return None
            
        except Exception as e:
            print(f"âŒ Error looking up collection: {e}")
            return None
    
    @staticmethod
    def export_to_zotero(markdown_path: Path, source_file: str,
                        qwen_model: str, pages_processed: int,
                        title: Optional[str] = None,
                        collection_key: Optional[str] = None) -> Optional[str]:
        """
        Export processed text to Zotero library with metadata and markdown attachment
        
        Returns:
            Zotero item key on success, None on failure
        """
        api_key = os.getenv("ZOTERO_API_KEY")
        user_id = os.getenv("ZOTERO_USER_ID")
        
        if not api_key or not user_id:
            print("âš ï¸  Warning: ZOTERO_API_KEY or ZOTERO_USER_ID not set. Skipping Zotero export.")
            return None
        
        try:
            # Prepare metadata
            source_name = Path(source_file).stem
            display_title = title if title else source_name
            process_date = datetime.now().strftime('%Y-%m-%d')
            
            # Create Zotero item
            item_data = {
                "itemType": "book",
                "title": display_title,
                "abstractNote": f"OCR-processed classical Chinese text from {Path(source_file).name}",
                "date": process_date,
                "language": "zh",
                "extra": (
                    f"OCR Engine: Qwen-VL ({qwen_model})\n"
                    f"Source File: {Path(source_file).name}\n"
                    f"Pages Processed: {pages_processed}\n"
                    f"Processing Date: {process_date}"
                )
            }
            
            # Add to collection if specified
            if collection_key:
                item_data["collections"] = [collection_key]
            
            # Create item in Zotero
            headers = {
                "Zotero-API-Key": api_key,
                "Content-Type": "application/json"
            }
            
            response = requests.post(
                f"https://api.zotero.org/users/{user_id}/items",
                headers=headers,
                json=[item_data]
            )
            
            if response.status_code != 200:
                print(f"âŒ Failed to create Zotero item: {response.status_code}")
                print(response.text)
                return None
            
            # Get the created item key
            item_key = response.json()['successful']['0']['key']
            print(f"âœ“ Created Zotero item: {item_key}")
            
            # Attach markdown file
            with open(markdown_path, 'rb') as f:
                file_content = f.read()
            
            attachment_response = requests.post(
                f"https://api.zotero.org/users/{user_id}/items/{item_key}/file",
                headers={
                    "Zotero-API-Key": api_key,
                    "Content-Type": "text/markdown",
                    "If-None-Match": "*"
                },
                data=file_content
            )
            
            if attachment_response.status_code == 200:
                print(f"âœ“ Attached markdown file to Zotero item")
            else:
                print(f"âš ï¸  Warning: Failed to attach file: {attachment_response.status_code}")
            
            return item_key
            
        except Exception as e:
            print(f"âŒ Error exporting to Zotero: {e}")
            import traceback
            traceback.print_exc()
            return None

# ============================================================================
# OCR ENGINE
# ============================================================================

class OCREngine:
    """Handles Qwen-VL OCR operations"""

    def __init__(self, clients: APIClients, config: ProcessingConfig):
        self.clients = clients
        self.config = config
        self.text_processor = TextProcessor()

    def process_image(self, image_path: str, page_num: int, total_pages: int) -> Optional[Tuple[str, bool]]:
        """Process a single image and return text with loop detection flag"""
        progress = f"({page_num}/{total_pages}, {page_num*100//total_pages}%)" if total_pages > 1 else ""
        print(f"\n--- Page {page_num} {progress} ---")

        # Preprocess image
        processed_path = ImageProcessor.preprocess_image(image_path, self.config)
        temp_preprocessed = processed_path if processed_path != image_path else None

        try:
            # Read and encode image as base64
            with open(processed_path, 'rb') as image_file:
                image_content = base64.b64encode(image_file.read()).decode('utf-8')

            # API call with retry logic
            text = None
            for attempt in range(self.config.retry_attempts):
                try:
                    response = self.clients.qwen.chat.completions.create(
                        model=self.config.qwen_model,
                        messages=[{
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": (
                                       "ä½ æ˜¯ä¸€ä¸ªå¤å…¸æ–‡çŒ®OCRä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹è¦æ±‚æå–å›¾ç‰‡ä¸­çš„å…¨éƒ¨ä¸­æ–‡æ–‡å­—:\n"
                                        "1. ç‰ˆé¢æ ¼å¼:ä¼ ç»Ÿç«–æ’ç‰ˆå¼,ä»å³åˆ°å·¦ã€ä»ä¸Šåˆ°ä¸‹é˜…è¯»\n"
                                        "2. æ¯åˆ—çº¦1-25å­—,å®Œæ•´æå–æ¯ä¸€åˆ—\n"
                                        "3. ä¿ç•™æ‰€æœ‰æ ‡ç‚¹ã€ç©ºæ ¼å’Œæ¢è¡Œ\n"
                                        "4. å¿½ç•¥é¡µç ã€å°ç« ã€æ°´å°ã€å›¾ä¹¦é¦†æ ‡è®°ç­‰å…ƒæ•°æ®ã€‚è‹¥é¡µé¢æ–‡å­—å†…å®¹å°‘äº10ä¸ªå­—ç¬¦ï¼Œæˆ–ä¸»è¦åŒºåŸŸä¸ºå›¾ç”»/å›¾è¡¨ï¼Œè¾“å‡º'[å›¾]ã€‚è‹¥é¡µé¢å®Œå…¨æ— æ–‡å­—å†…å®¹ï¼Œæˆ–ä»…æœ‰ä»…æœ‰é¡µç /å›¾ä¹¦é¦†æ ‡è®°ï¼Œè¾“å‡º'[ç©ºé¡µ]'\n"
                                        "5. å¦‚æœé‡åˆ°æ¨¡ç³Šæˆ–ç ´æŸæ–‡å­—,ç”¨ã€?ã€‘æ ‡æ³¨\n"
                                        "6. åªæå–é¡µé¢ä¸»ä½“æ–‡å­—åŒºåŸŸ,å¿½ç•¥è£…è®¢è¾¹å’Œé¡µè¾¹è·\n"
                                        "7. è¾“å‡ºçº¯æ–‡æœ¬,ä¸è¦æ·»åŠ ä»»ä½•è¯´æ˜æˆ–æ ‡é¢˜\n"
                                        "8. ç‰¹æ®Šæ ¼å¼å¤„ç†ï¼š å¦‚é‡è¡¨æ ¼ã€åå†Œã€è´¦ç›®ç­‰è¡Œåˆ—å¯¹é½çš„ç‰ˆå¼ï¼Œè¯·ä¼˜å…ˆä¿æŒå…¶è¡Œåˆ—ç»“æ„ã€‚å¯ä½¿ç”¨æ¢è¡Œå’Œç©ºæ ¼æ¥åŒºåˆ†ä¸åŒæ¡ç›®ï¼Œç¡®ä¿åŒä¸€è¡Œçš„æ•°æ®ä¿æŒåœ¨åŒä¸€è¡Œ\n"
                                        "9. ä¸¥æ ¼ä¿æŒåŸæ–‡ç”¨å­—ï¼šå¦‚åŸæ–‡ä¸ºç¹ä½“å­—ï¼Œè¾“å‡ºä¸€å¾‹ä½¿ç”¨ç¹ä½“ï¼›ä»…å½“åŸæ–‡ç¡®ä¸ºç®€åŒ–å­—æ—¶æ–¹ç”¨ç®€ä½“"
                                        "\nè¯·å¼€å§‹æå–:"
                                    )
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {"url": f"data:image/png;base64,{image_content}"}
                                }
                            ]
                        }],
                        max_tokens=4096,
                        temperature=0.1,
                        timeout=self.config.api_timeout
                    )
                    
                    text = response.choices[0].message.content.strip()
                    break  # Success, exit retry loop
                    
                except Exception as e:
                    if attempt < self.config.retry_attempts - 1:
                        wait_time = self.config.retry_delay * (2 ** attempt)
                        print(f"  âš ï¸  OCR error (attempt {attempt+1}/{self.config.retry_attempts}): {e}")
                        print(f"  Retrying in {wait_time}s...")
                        time.sleep(wait_time)
                    else:
                        print(f"  âŒ All OCR retry attempts failed: {e}")
                        return None

            if text is None:
                return None

            char_count = len(text)

            # Progressive quality checks with our agreed thresholds
            if char_count > self.config.expected_chars_normal:
                print(f"  âš ï¸  Long page: {char_count} chars (expected <{self.config.expected_chars_normal})")

            if char_count > self.config.ocr_soft_cap:
                print(f"  âš ï¸  Very long page: {char_count} chars (soft cap: {self.config.ocr_soft_cap})")

            if char_count > self.config.ocr_hard_cap:
                print(f"  ğŸ”´ Excessive length: {char_count} chars - truncating to {self.config.ocr_hard_cap}")
                text = text[:self.config.ocr_hard_cap]

            # Detect loops and first pass truncation
            has_loops = self.text_processor.detect_ocr_loops_simple(text)

            if has_loops:
                print(f"  ğŸ”„ Potential OCR loops detected - truncating aggressively")
                # Find first major repetition and cut there
                text = ImageProcessor.truncate_at_loop(text, max_length=800)

            print(f"  âœ“ OCR success ({len(text)} chars{', has loops' if has_loops else ''})")

            # Remove library metadata
            cleaned_text = self.text_processor.remove_metadata_text(text)

            return cleaned_text, has_loops

        finally:
            # Clean up temporary preprocessed image
            if temp_preprocessed and os.path.exists(temp_preprocessed):
                try:
                    os.unlink(temp_preprocessed)
                except:
                    pass

# ============================================================================
# TEXT CLEANING ENGINE
# ============================================================================

class TextCleaner:
    """Handles Kimi text cleaning operations"""

    def __init__(self, clients: APIClients, config: ProcessingConfig):
        self.clients = clients
        self.config = config
        self.text_processor = TextProcessor()

    def clean_chunk(self, text_chunk: str, context: str = "", has_ocr_loops: bool = False) -> str:
        """Clean a single chunk of text with retry logic"""
        start_time = time.time()
        loop_warning = ""
        if has_ocr_loops:
            loop_warning = "\n\n**é‡è¦æç¤º**ï¼šæ­¤æ–‡æœ¬å¯èƒ½åŒ…å«OCRè¯†åˆ«å¾ªç¯å¯¼è‡´çš„é‡å¤å†…å®¹ï¼Œè¯·ä»”ç»†æ£€æŸ¥å¹¶åˆ é™¤æ‰€æœ‰é‡å¤éƒ¨åˆ†ã€‚"

        prompt = f"""è¯·å°†ä»¥ä¸‹OCRæ–‡æœ¬æ•´ç†æˆè¿è´¯çš„æ–‡è¨€æ–‡æ–‡çŒ®ã€‚è¿™æ˜¯æ˜æ¸…æ—¶æœŸçš„æ–‡é›†ã€‚
è¦æ±‚ï¼š
1. ä¿®æ­£æ˜æ˜¾çš„OCRè¯†åˆ«é”™è¯¯ï¼ˆå½¢è¿‘å­—è¯¯è®¤ç­‰ï¼‰
2. æ·»åŠ é€‚å½“çš„æ ‡ç‚¹ç¬¦å·ï¼ˆå¥å·ã€é€—å·ç­‰ï¼‰ä½¿å…¶å¯è¯»
3. ä¿æŒåŸæ–‡çš„å¤æ±‰è¯­ç‰¹å¾ï¼Œä¸è¦ç°ä»£åŒ–
4. åˆ é™¤æ‰€æœ‰æœºæ„å…ƒæ•°æ®ï¼ˆå¦‚é¡µç ã€å›¾ä¹¦é¦†æ ‡è®°ç­‰ï¼‰
5. å°†æ–‡æœ¬æ•´ç†æˆè¿è´¯çš„æ®µè½
6. å¯¹äºæ— æ³•ç¡®å®šçš„å­—ï¼Œç”¨ã€ï¼Ÿã€‘æ ‡æ³¨
7. **ç‰¹åˆ«æ³¨æ„ï¼šå¦‚æœå‘ç°é‡å¤å‡ºç°çš„æ–‡æœ¬å—ï¼ˆOCRè¯†åˆ«å¾ªç¯é”™è¯¯ï¼‰ï¼Œè¯·åˆ é™¤è¿™äº›é‡å¤éƒ¨åˆ†**
8. **å¦‚æœæ–‡æœ¬æ˜æ˜¾ä¸å®Œæ•´æˆ–è¢«æˆªæ–­ï¼Œä¿æŒåŸæ ·å³å¯**{loop_warning}

{f"ä¸Šä¸‹æ–‡æç¤ºï¼š{context}" if context else ""}

OCRæ–‡æœ¬ï¼š
{text_chunk}

è¯·ç›´æ¥è¿”å›æ•´ç†åçš„æ–‡æœ¬ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è¯´æ˜ã€åˆ—è¡¨æˆ–æ ‡é¢˜ï¼Œç«‹å³å¼€å§‹æ­£æ–‡ï¼š"""

        for attempt in range(self.config.retry_attempts):
            try:
                response = self.clients.kimi.chat.completions.create(
                    model="kimi-k2-0905-preview",
                    messages=[{"role": "user", "content": prompt}],
                    timeout=self.config.api_timeout,
                    max_tokens=2048
                )
                processing_time = time.time() - start_time
                print(f"({processing_time:.1f}s)")
                return response.choices[0].message.content

            except Exception as e:
                print(f"âœ— Error (attempt {attempt+1}/{self.config.retry_attempts}): {e}")
                if attempt < self.config.retry_attempts - 1:
                    wait_time = self.config.retry_delay * (2 ** attempt)
                    print(f"    Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
                else:
                    print("    All retry attempts failed. Returning original text.")
                    return text_chunk

        return text_chunk

    def create_chunks_with_overlap(self, text: str) -> List[Tuple[str, int, int]]:
        """Create overlapping chunks for processing"""
        chunks = []
        start = 0

        while start < len(text):
            end = min(start + self.config.max_text_length, len(text))
            chunk = text[start:end]
            chunks.append((chunk, start, end))
            start = end - self.config.chunk_overlap

        print(f"Created {len(chunks)} chunks with {self.config.chunk_overlap}-char overlap")
        return chunks

    def process_text_in_chunks(self, text: str, context: str = "", has_ocr_loops: bool = False) -> str:
        """Process text in chunks with overlap"""
        if len(text) <= self.config.max_text_length:
            print("Processing single chunk...", end=' ')
            return self.clean_chunk(text, context, has_ocr_loops)

        print(f"\nText length: {len(text)} chars")
        chunks = self.create_chunks_with_overlap(text)
        cleaned_chunks = []

        for i, (chunk, start, end) in enumerate(chunks, 1):
            print(f"\nProcessing chunk {i}/{len(chunks)} (chars {start}-{end})...", end=' ')
            cleaned = self.clean_chunk(chunk, context, has_ocr_loops)
            cleaned_chunks.append(cleaned)

        combined = '\n\n'.join(cleaned_chunks)
        print(f"\nâœ“ Combined all chunks into {len(combined)} chars")
        return combined

    def final_polish(self, text: str, context: str = "") -> str:
        """Final polish pass on the complete text"""
        print("\n=== Final Polish Pass ===")
        prompt = f"""è¯·å¯¹ä»¥ä¸‹å·²æ•´ç†çš„æ–‡è¨€æ–‡è¿›è¡Œæœ€ç»ˆæ¶¦è‰²ã€‚è¿™æ˜¯æ˜æ¸…æ—¶æœŸçš„æ–‡çŒ®ã€‚

è¦æ±‚ï¼š
1. ç§»é™¤Kimiå¯èƒ½æ·»åŠ çš„ä»»ä½•è¯´æ˜æ€§æ–‡å­—ï¼ˆå¦‚"æ”¹å†™è¯´æ˜"ã€"æ¶¦è‰²è¯´æ˜"ç­‰ï¼‰
2. ä¿®æ­£å‰©ä½™çš„æ˜æ˜¾é”™è¯¯
3. ç¡®ä¿æ ‡ç‚¹ç¬¦å·æ­£ç¡®ä¸”ä¸€è‡´
4. ç¡®ä¿æ®µè½ä¹‹é—´æœ‰é€‚å½“çš„è¿è´¯æ€§
5. åˆ é™¤ä»»ä½•é‡å¤çš„æ®µè½æˆ–æ–‡æœ¬å—
6. ä¿æŒåŸæ–‡çš„å¤æ±‰è¯­ç‰¹å¾
7. **ä¸è¦æ·»åŠ ä»»ä½•æ–°çš„è¯´æ˜ã€æ ‡é¢˜æˆ–å…ƒæ•°æ®**
8. **ç›´æ¥è¾“å‡ºæ¶¦è‰²åçš„æ­£æ–‡ï¼Œä¸è¦æœ‰ä»»ä½•å‰è¨€æˆ–åè®°**

{f"ä¸Šä¸‹æ–‡æç¤ºï¼š{context}" if context else ""}

æ–‡æœ¬ï¼š
{text}

è¯·ç›´æ¥è¾“å‡ºæ¶¦è‰²åçš„å®Œæ•´æ–‡æœ¬ï¼š"""

        for attempt in range(self.config.retry_attempts):
            try:
                response = self.clients.kimi.chat.completions.create(
                    model="kimi-k2-0905-preview",
                    messages=[{"role": "user", "content": prompt}],
                    timeout=self.config.api_timeout,
                    max_tokens=4096
                )
                polished = response.choices[0].message.content
                print("âœ“ Final polish complete")
                return self.text_processor.remove_explanatory_text(polished)

            except Exception as e:
                print(f"âœ— Polish error (attempt {attempt+1}/{self.config.retry_attempts}): {e}")
                if attempt < self.config.retry_attempts - 1:
                    wait_time = self.config.retry_delay * (2 ** attempt)
                    print(f"    Retrying in {wait_time} seconds...")
                    time.sleep(wait_time)
                else:
                    print("    All retry attempts failed. Returning unpolished text.")
                    return text

        return text

# ============================================================================
# DOCUMENT PROCESSOR
# ============================================================================

class DocumentProcessor:
    """Main document processing orchestrator"""

    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.clients = APIClients()
        self.ocr_engine = OCREngine(self.clients, config)
        self.text_cleaner = TextCleaner(self.clients, config)
        self.zotero = ZoteroExporter()

    def process_image(self, image_path: str, context: str = "",
                     output_dir: str = "./processed",
                     quick_mode: bool = False) -> Optional[Path]:
        """Process a single image file"""
        print(f"\n{'='*60}")
        print(f"Processing image: {image_path}")
        print(f"{'='*60}")

        # OCR
        result = self.ocr_engine.process_image(image_path, 1, 1)
        if not result:
            print("âŒ OCR failed")
            return None

        raw_text, has_loops = result

        # Save raw OCR
        os.makedirs(output_dir, exist_ok=True)
        doc_name = Path(image_path).stem

        # Clean text
        print("\n=== Text Cleaning Stage ===")
        cleaned_text = self.text_cleaner.process_text_in_chunks(raw_text, context, has_loops)

        # Final polish
        if not quick_mode:
            cleaned_text = self.text_cleaner.final_polish(cleaned_text, context)

        # Create consolidated note
        pages_with_loops = [1] if has_loops else []

        output_path = self._create_consolidated_note(
            doc_name, cleaned_text, pages_with_loops,
            output_dir, context, 1
        )

        print(f"\nâœ“ Processing complete: {output_path}")
        return output_path

    def process_pdf(self, pdf_path: str, context: str = "",
                   output_dir: str = "./processed",
                   quick_mode: bool = False, max_pages: Optional[int] = None,
                   start_page: int = 1,
                   export_zotero: bool = False,
                   zotero_title: Optional[str] = None,
                   zotero_collection: Optional[str] = None) -> Optional[Path]:
        """Process a PDF file"""
        print(f"\n{'='*60}")
        print(f"Processing PDF: {pdf_path}")
        print(f"Output directory: {output_dir}")
        print(f"{'='*60}")

        if not PDF_SUPPORT:
            print("âŒ PDF support not available")
            return None

        try:
            # Get page info
            info = pdfinfo_from_path(pdf_path)
            total_pages = info["Pages"]
            end_page = min(start_page + max_pages - 1, total_pages) if max_pages else total_pages
            page_count = end_page - start_page + 1

            print(f"Total PDF pages: {total_pages}")
            print(f"Processing pages {start_page} to {end_page} ({page_count} pages)")
            print(f"DPI: {self.config.ocr_dpi}, Model: {self.config.qwen_model}")

            # Create output directory
            os.makedirs(output_dir, exist_ok=True)
            doc_name = Path(pdf_path).stem

            # OCR Stage
            print("\n" + "="*60)
            print("STAGE 1: OCR EXTRACTION")
            print("="*60)

            all_ocr_texts = []
            pages_with_loops = []
            start_time = time.time()

            for page_num in range(start_page, end_page + 1):
                # Convert single page
                images = convert_from_path(
                    pdf_path,
                    dpi=self.config.ocr_dpi,
                    first_page=page_num,
                    last_page=page_num
                )

                if not images:
                    print(f"âš ï¸  Skipping page {page_num} - conversion failed")
                    continue

                # Save as temporary image
                temp_image = tempfile.NamedTemporaryFile(suffix='.png', delete=False)
                images[0].save(temp_image.name, 'PNG')

                try:
                    # Process with OCR
                    result = self.ocr_engine.process_image(temp_image.name, page_num, page_count)
                    if result:
                        text, has_loops = result
                        all_ocr_texts.append(text)
                        if has_loops:
                            pages_with_loops.append(page_num)
                finally:
                    # Clean up temp image
                    if os.path.exists(temp_image.name):
                        os.unlink(temp_image.name)

            ocr_time = time.time() - start_time

            if not all_ocr_texts:
                print("âŒ No text extracted from any pages")
                return None

            # Combine OCR texts
            combined_ocr = '\n\n'.join(all_ocr_texts)
            print(f"\nâœ“ OCR complete: {len(combined_ocr)} chars from {len(all_ocr_texts)} pages")
            print(f"   Time: {ocr_time:.1f}s")
            print(f"   Pages with loops: {len(pages_with_loops)}")

            # Save OCR backup
            backup_data = {
                "document": doc_name,
                "source": str(pdf_path),
                "pages_processed": page_count,
                "pages_with_loops": pages_with_loops,
                "raw_ocr_text": combined_ocr,
                "context": context,
                "timestamp": datetime.now().isoformat(),
                "config": {
                    "model": self.config.qwen_model,
                    "dpi": self.config.ocr_dpi,
                }
            }

            backup_path = Path(output_dir) / f"{doc_name}_raw_ocr_latest.json"
            with open(backup_path, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, ensure_ascii=False, indent=2)
            print(f"   Backup saved: {backup_path}")

            # Cleaning Stage
            print("\n" + "="*60)
            print("STAGE 2: TEXT CLEANING")
            print("="*60)

            has_loops = len(pages_with_loops) > 0
            cleaned_text = self.text_cleaner.process_text_in_chunks(
                combined_ocr, context, has_loops
            )

            # Final polish
            if not quick_mode:
                cleaned_text = self.text_cleaner.final_polish(cleaned_text, context)

            # Create consolidated note
            output_path = self._create_consolidated_note(
                doc_name, cleaned_text, pages_with_loops,
                output_dir, context, page_count
            )

            total_time = time.time() - start_time
            print(f"\n{'='*60}")
            print(f"âœ“ Processing complete in {total_time:.1f}s")
            print(f"âœ“ Output: {output_path}")
            print(f"{'='*60}")

            # Export to Zotero if requested
            if export_zotero:
                print(f"\nğŸ“š Exporting to Zotero...")
                collection_key = None
                if zotero_collection:
                    collection_key = self.zotero.get_collection_key(zotero_collection)
                
                self.zotero.export_to_zotero(
                    output_path,
                    pdf_path,
                    self.config.qwen_model,
                    page_count,
                    title=zotero_title,
                    collection_key=collection_key
                )

            return output_path

        except Exception as e:
            print(f"âŒ Error processing PDF: {e}")
            import traceback
            traceback.print_exc()
            return None

    def resume_from_backup(self, backup_path: str, context: str = "",
                          output_dir: str = "./processed",
                          quick_mode: bool = False,
                          export_zotero: bool = False,
                          zotero_title: Optional[str] = None,
                          zotero_collection: Optional[str] = None):
        """Resume processing from a raw OCR backup file"""
        print(f"\n{'='*60}")
        print(f"Resuming from backup: {backup_path}")
        print(f"{'='*60}")

        try:
            with open(backup_path, 'r', encoding='utf-8') as f:
                backup_data = json.load(f)

            doc_name = backup_data["document"]
            combined_ocr = backup_data["raw_ocr_text"]
            pages_with_loops = backup_data.get("pages_with_loops", [])
            page_count = backup_data.get("pages_processed", 0)
            saved_context = backup_data.get("context", "")

            # Use provided context or fall back to saved context
            final_context = context if context else saved_context

            print(f"Document: {doc_name}")
            print(f"Text length: {len(combined_ocr)} chars")
            print(f"Pages with loops: {len(pages_with_loops)}")

            # Cleaning Stage
            print("\n" + "="*60)
            print("STAGE 2: TEXT CLEANING (RESUMED)")
            print("="*60)

            has_loops = len(pages_with_loops) > 0
            cleaned_text = self.text_cleaner.process_text_in_chunks(
                combined_ocr, final_context, has_loops
            )

            # Final polish
            if not quick_mode:
                cleaned_text = self.text_cleaner.final_polish(cleaned_text, final_context)

            # Create consolidated note
            output_path = self._create_consolidated_note(
                doc_name, cleaned_text, pages_with_loops,
                output_dir, final_context, page_count
            )

            print(f"\nâœ“ Resume processing complete: {output_path}")

            # Export to Zotero if requested
            if export_zotero:
                print(f"\nğŸ“š Exporting to Zotero...")
                collection_key = None
                if zotero_collection:
                    collection_key = self.zotero.get_collection_key(zotero_collection)
                
                source_file = backup_data.get("source", doc_name)
                self.zotero.export_to_zotero(
                    output_path,
                    source_file,
                    self.config.qwen_model,
                    page_count,
                    title=zotero_title,
                    collection_key=collection_key
                )

            return output_path

        except Exception as e:
            print(f"âŒ Error resuming from backup: {e}")
            import traceback
            traceback.print_exc()
            return None

    def _create_consolidated_note(self, document_name: str, full_cleaned_text: str,
                                pages_with_loops: List[int],
                                output_dir: str, context: str, page_count: int) -> Path:
        """Create consolidated markdown note"""
        collection = Path(output_dir).name if output_dir != "./processed" else Path(document_name).stem

        # Generate quality section if there were problematic pages
        quality_section = ""
        if pages_with_loops:
            quality_section = f"""
## âš ï¸ OCRè´¨é‡æç¤º

æ£€æµ‹åˆ° {len(pages_with_loops)} é¡µå¯èƒ½åŒ…å«OCRå¾ªç¯é”™è¯¯ï¼Œå·²æ ‡è®°ä¾›Kimiæ¸…ç†ï¼š

**éœ€è¦æ£€æŸ¥çš„é¡µé¢**: {', '.join(map(str, pages_with_loops[:20]))}
{f"...åŠå…¶ä»– {len(pages_with_loops)-20} é¡µ" if len(pages_with_loops) > 20 else ""}

è¿™äº›é¡µé¢çš„é‡å¤å†…å®¹åº”è¯¥å·²è¢«è‡ªåŠ¨æ¸…ç†ï¼Œä½†å»ºè®®äººå·¥å¤æ ¸ã€‚

---
"""

        # Build the document
        note_content = f"""---
type: primary-source
source: {document_name}
collection: {collection}
date-processed: {datetime.now().strftime('%Y-%m-%d')}
total-pages: {page_count}
flagged-pages: {len(pages_with_loops)}
status: {"needs-review" if pages_with_loops else "clean"}
ocr-engine: Qwen-VL
tags:
- primary-source
- {collection}
- Ming-Qing
---

# {document_name}

> [!info] Document Metadata
> - **Collection**: {collection}
> - **Total Pages**: {page_count}
> - **Flagged Pages**: {len(pages_with_loops)}/{page_count}
> - **Processing Date**: {datetime.now().strftime('%Y-%m-%d %H:%M')}
> - **OCR Engine**: Qwen-VL ({self.config.qwen_model})
> - **Context**: {context if context else "None"}
> - **Status**: {"ğŸŸ¡ Needs Review" if pages_with_loops else "ğŸŸ¢ Clean"}

---

{quality_section}

## å…¨æ–‡æ•´ç†

{full_cleaned_text}
"""

        # Add research notes section
        note_content += f"""
---

## ç ”ç©¶ç¬”è®°

### å…³é”®æœ¯è¯­
-

### å†å²è¯­å¢ƒ
-

### ç›¸å…³æ–‡çŒ®
-

---

## å¤„ç†å†å²
- {datetime.now().strftime('%Y-%m-%d')}: OCR (Qwen-VL) and initial processing
- Pages processed: {page_count}
- Model used: {self.config.qwen_model}
- Pages with loops: {len(pages_with_loops)}
"""

        # Save file
        output_path = Path(output_dir) / f"{document_name}.md"
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(note_content)

        print(f"\nâœ“ Created consolidated note: {output_path}")
        return output_path

# ============================================================================
# COMMAND LINE INTERFACE
# ============================================================================

def create_argument_parser() -> argparse.ArgumentParser:
    """Create command line argument parser"""
    parser = argparse.ArgumentParser(
        description='Process classical Chinese documents with Qwen-VL and Kimi',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Process single image
  %(prog)s scan.jpg --output ./processed

  # Process PDF with context
  %(prog)s document.pdf --context "æ˜ä»£æ–‡é›†ï¼Œç«–æ’æ— æ ‡ç‚¹" --output ~/Obsidian/Primary-Sources/

  # Quick mode (skip final polish)
  %(prog)s document.pdf --quick --output ./processed

  # Use faster/cheaper model
  %(prog)s document.pdf --model qwen-vl-plus --output ./processed

  # Process pages 50-100 only
  %(prog)s document.pdf --start-page 50 --max-pages 51 --output ./processed

  # Process from page 100 to end
  %(prog)s document.pdf --start-page 100 --output ./processed

  # Limit pages for testing
  %(prog)s document.pdf --max-pages 5 --output ./processed

  # Resume from OCR backup
  %(prog)s --resume-from ./processed/document_raw_ocr_latest.json --output ./processed

  # Export to Zotero
  %(prog)s document.pdf --zotero --zotero-collection "Primary Sources" --output ./processed

  # Batch process directory
  %(prog)s ./scans/ --batch --context "æ³‰å·åºœå¿—" --output ~/Obsidian/Primary-Sources/
"""
    )

    parser.add_argument('input', nargs='?', help='Image file, PDF, or directory (not required with --resume-from)')
    parser.add_argument('--context', default='', help='Contextual information')
    parser.add_argument('--output', default='./processed', help='Output directory')
    parser.add_argument('--batch', action='store_true', help='Process all files in directory')
    parser.add_argument('--dpi', type=int, default=200, help='DPI for PDF conversion')
    parser.add_argument('--model', default='qwen-vl-max', choices=['qwen-vl-plus', 'qwen-vl-max'], help='Qwen model')
    parser.add_argument('--quick', action='store_true', help='Skip final polish step')
    parser.add_argument('--max-pages', type=int, help='Limit processing to first N pages')
    parser.add_argument('--start-page', type=int, default=1, help='Start processing from page N (default: 1)')
    parser.add_argument('--resume-from', help='Resume from raw OCR JSON file')
    parser.add_argument('--zotero', action='store_true', help='Export to Zotero after processing')
    parser.add_argument('--zotero-title', help='Custom title for Zotero item (defaults to filename)')
    parser.add_argument('--zotero-collection', help='Zotero collection name to file the item in')

    return parser

def main():
    """Main entry point"""
    parser = create_argument_parser()
    args = parser.parse_args()

    # Validate API keys
    if not APIClients.validate_keys():
        sys.exit(1)

    # Initialize configuration
    config = ProcessingConfig()
    config.update_from_args(args)

    # Initialize processor
    processor = DocumentProcessor(config)

    # Handle resume case
    if args.resume_from:
        if not Path(args.resume_from).exists():
            print(f"âŒ Error: Backup file {args.resume_from} not found")
            sys.exit(1)
        processor.resume_from_backup(
            args.resume_from, args.context, args.output, args.quick,
            export_zotero=args.zotero,
            zotero_title=args.zotero_title,
            zotero_collection=args.zotero_collection
        )
        return

    # Validate input
    if not args.input:
        print("âŒ Error: Input file or directory required when not using --resume-from")
        sys.exit(1)

    # Batch processing
    if args.batch:
        input_path = Path(args.input)
        if not input_path.is_dir():
            print(f"âŒ Error: {args.input} is not a directory")
            sys.exit(1)

        images = list(input_path.glob('*.jpg')) + list(input_path.glob('*.png')) + list(input_path.glob('*.jpeg'))
        pdfs = list(input_path.glob('*.pdf'))

        if not images and not pdfs:
            print(f"âŒ No images or PDFs found in {args.input}")
            sys.exit(1)

        print(f"Found {len(images)} image(s) and {len(pdfs)} PDF(s) to process\n")
        print("=" * 60)

        successful = 0
        failed = 0

        for img in images:
            result = processor.process_image(str(img), args.context, args.output, args.quick)
            if result:
                successful += 1
            else:
                failed += 1

        for pdf in pdfs:
            result = processor.process_pdf(
                str(pdf), args.context, args.output, args.quick, 
                args.max_pages, args.start_page,
                export_zotero=args.zotero,
                zotero_title=args.zotero_title,
                zotero_collection=args.zotero_collection
            )
            if result:
                successful += 1
            else:
                failed += 1

        print("=" * 60)
        print(f"\nğŸ“Š Processing complete:")
        print(f"   âœ“ Successful: {successful}")
        print(f"   âœ— Failed: {failed}")

    else:
        # Single file processing
        if not Path(args.input).exists():
            print(f"âŒ Error: File {args.input} not found")
            sys.exit(1)

        file_ext = Path(args.input).suffix.lower()
        if file_ext == '.pdf':
            if not PDF_SUPPORT:
                print("âŒ Error: PDF support not available")
                print("   Install with: pip install pdf2image")
                sys.exit(1)
            processor.process_pdf(
                args.input, args.context, args.output, args.quick, 
                args.max_pages, args.start_page,
                export_zotero=args.zotero,
                zotero_title=args.zotero_title,
                zotero_collection=args.zotero_collection
            )
        else:
            processor.process_image(args.input, args.context, args.output, args.quick)

        print("\nâœ“ Processing complete!")

if __name__ == "__main__":
    main()

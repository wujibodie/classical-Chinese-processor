#!/usr/bin/env python3
"""
Classical Chinese Document Processor (Qwen-VL + Kimi)
OCR with Qwen-VL, cleaning with Kimi
Outputs single consolidated markdown file with coherent text
"""

import sys
import os
import json
import re
import time
import tempfile
import base64
import argparse
import concurrent.futures
import threading
from pathlib import Path
from datetime import datetime
from typing import List, Tuple, Optional, Dict, Any
from dataclasses import dataclass

# Third-party imports
import requests
from openai import OpenAI

# Optional dependencies
try:
    from pdf2image import convert_from_path, pdfinfo_from_path
    PDF_SUPPORT = True
except ImportError:
    PDF_SUPPORT = False
    print("âš ï¸  Warning: pdf2image not installed. PDF support disabled.", flush=True)

try:
    import cv2
    import numpy as np
    CV2_AVAILABLE = True
except ImportError:
    CV2_AVAILABLE = False
    print("âš ï¸  Warning: opencv-python not installed. Image preprocessing disabled.", flush=True)

# ============================================================================
# CONFIGURATION MANAGEMENT
# ============================================================================

@dataclass
class ProcessingConfig:
    """Centralized configuration for document processing"""
    # OCR Settings
    ocr_dpi: int = 300
    qwen_model: str = "qwen3-vl-plus"
    max_image_width: int = 3072

    # Cleanup Settings
    cleanup_model: str = "deepseek"  # "kimi" or "deepseek"

    # OCR Quality Thresholds
    expected_chars_normal: int = 500   # Flag if above this
    ocr_soft_cap: int = 800           # Warn if above this
    ocr_hard_cap: int = 1200          # Truncate if above this

    # Text Processing Settings
    max_text_length: int = 2400
    chunk_overlap: int = 400
    api_timeout: int = 300
    retry_attempts: int = 3
    retry_delay: int = 2

    # Model Costs
    model_costs: Dict[str, float] = None

    def __post_init__(self):
        if self.model_costs is None:
            self.model_costs = {
                "qwen3-vl-plus": 0.004,
                "qwen-vl-max": 0.008,
            }

    def update_from_args(self, args):
        """Update config from command line arguments"""
        if hasattr(args, 'dpi'):
            self.ocr_dpi = args.dpi
        if hasattr(args, 'model'):
            self.qwen_model = args.model
        if hasattr(args, 'cleanup_model'):
            self.cleanup_model = args.cleanup_model

# ============================================================================
# CLIENT MANAGEMENT
# ============================================================================

class APIClients:
    """Manages API clients for different services"""

    def __init__(self, cleanup_model: str = "kimi"):
        if not self.validate_keys(cleanup_model):
            raise Exception("API keys not configured")

        self.kimi = None
        self.deepseek = None

        # Initialize cleanup model client
        if cleanup_model == "kimi":
            self.kimi = OpenAI(
                api_key=os.getenv("KIMI_API_KEY"),
                base_url="https://api.moonshot.cn/v1"
            )
        elif cleanup_model == "deepseek":
            self.deepseek = OpenAI(
                api_key=os.getenv("DEEPSEEK_API_KEY"),
                base_url="https://api.deepseek.com"
            )

        self.qwen = OpenAI(
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url="https://dashscope-intl.aliyuncs.com/compatible-mode/v1",
        )

    @staticmethod
    def validate_keys(cleanup_model: str = "kimi"):
        """Validate that required API keys are set"""
        if not os.getenv("DASHSCOPE_API_KEY"):
            print("âŒ Error: DASHSCOPE_API_KEY environment variable not set", flush=True)
            return False

        if cleanup_model == "kimi" and not os.getenv("KIMI_API_KEY"):
            print("âŒ Error: KIMI_API_KEY environment variable not set", flush=True)
            return False

        if cleanup_model == "deepseek" and not os.getenv("DEEPSEEK_API_KEY"):
            print("âŒ Error: DEEPSEEK_API_KEY environment variable not set", flush=True)
            return False

        return True

# ============================================================================
# TEXT PROCESSING UTILITIES
# ============================================================================

class TextProcessor:
    """Handles text validation and cleaning operations"""

    @staticmethod
    def detect_ocr_loops_simple(text: str, sample_size: int = 200, max_repeats: int = 3) -> bool:
        """Quick check for obvious OCR repetition artifacts"""
        if len(text) < 1000:  # Only check longer texts
            return False

        # Sample a few spots for repetition
        check_length = min(len(text), 2000)
        for i in range(0, check_length - 100, sample_size):
            sample = text[i:i+100]
            if len(sample) < 100:
                continue
            if text.count(sample) > max_repeats:
                return True
        return False

    @staticmethod
    def remove_metadata_text(text: str) -> str:
        """Remove common institutional metadata from OCR text"""
        patterns = [
            r'å›½ç«‹å…¬æ–‡æ›¸é¤¨', r'National Archives of Japan', r'å…§é–£æ–‡åº«',
            r'ç•ªè™Ÿ\s*æ¼¢?\s*\d+', r'å†Šæ•¸\s*\d+', r'å·è™Ÿ\s*\d+',
            r'colorchecker', r'Kodak Gray Scale', r'x-rite',
            r'MSCCPPCC\d+', r'MSCCPPPE\d+', r'Kodak, 2007 TM: Kodak',
            r'ã€”.*?ã€•',
        ]

        for pattern in patterns:
            text = re.sub(pattern, '', text, flags=re.IGNORECASE)

        # Clean up lines
        lines = text.split('\n')
        cleaned_lines = []
        for line in lines:
            line = line.strip()
            if line and not any(keyword in line for keyword in ['ç¶´å¿ƒéƒ¨', 'æ–‡å­—ç‚ºé–‹ä¸é®®æ˜']):
                cleaned_lines.append(line)

        return '\n'.join(cleaned_lines)

    @staticmethod
    def remove_explanatory_text(text: str) -> str:
        """Remove any explanatory notes added by AI"""
        patterns = [
            r'---\s*\n\*\*æ”¹å†™è¯´æ˜\*\*.*?(?=---|\Z)',
            r'ã€”æ”¹å†™è¯´æ˜ã€•.*?ã€”/æ”¹å†™è¯´æ˜ã€•',
            r'ã€”æ¶¦è‰²è¯´æ˜ã€•.*?ã€”/æ¶¦è‰²è¯´æ˜ã€•',
            r'\næ³¨ï¼š.*', r'\nè¯´æ˜ï¼š.*',
        ]

        for pattern in patterns:
            text = re.sub(pattern, '', text, flags=re.DOTALL)

        # Remove standalone explanatory lines
        lines = text.split('\n')
        cleaned_lines = []
        skip_next = False

        for line in lines:
            line = line.strip()
            if any(keyword in line for keyword in ['æ”¹å†™è¯´æ˜', 'æ¶¦è‰²è¯´æ˜', 'è°ƒæ•´è¯´æ˜']):
                skip_next = True
                continue
            if skip_next and line.startswith('---'):
                skip_next = False
                continue
            if not skip_next and line:
                cleaned_lines.append(line)

        return '\n'.join(cleaned_lines)

# ============================================================================
# IMAGE PROCESSING
# ============================================================================

class ImageProcessor:
    """Handles image preprocessing operations"""

    @staticmethod
    def preprocess_image(image_path: str, config: ProcessingConfig) -> str:
        """Improve OCR quality by preprocessing image"""
        if not CV2_AVAILABLE:
            return image_path

        try:
            img = cv2.imread(image_path)
            if img is None:
                return image_path

            # Convert to grayscale
            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

            # Resize if too large
            h, w = gray.shape
            if w > config.max_image_width:
                ratio = config.max_image_width / w
                new_h = int(h * ratio)
                gray = cv2.resize(gray, (config.max_image_width, new_h), interpolation=cv2.INTER_AREA)
                print(f"    Resized image from {w}x{h} to {config.max_image_width}x{new_h}", flush=True)

            # Denoise and binarize
            denoised = cv2.fastNlMeansDenoising(gray, h=10, templateWindowSize=7, searchWindowSize=21)
            _, binary = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

            # Save preprocessed image
            temp_file = tempfile.NamedTemporaryFile(suffix='_preprocessed.png', delete=False)
            cv2.imwrite(temp_file.name, binary)
            return temp_file.name

        except Exception as e:
            print(f"Preprocessing error: {e}", flush=True)
            return image_path

    @staticmethod
    def truncate_at_loop(text: str, max_length: int = 500) -> str:
        """Truncate text at first major repetition pattern"""
        for i in range(0, min(len(text), max_length), 50):
            sample = text[i:i+100]
            if len(sample) < 100:
                continue
            # If this 100-char chunk appears again, cut before second occurrence
            second_occurrence = text.find(sample, i+100)
            if second_occurrence != -1:
                print(f"    âœ‚ï¸ Cutting at char {second_occurrence} (loop detected)", flush=True)
                return text[:second_occurrence]
        return text[:max_length]

# ============================================================================
# ZOTERO INTEGRATION
# ============================================================================

class ZoteroExporter:
    """Handles exporting to Zotero"""
    
    @staticmethod
    def get_collection_key(collection_name: str) -> Optional[str]:
        """Look up Zotero collection key by name"""
        api_key = os.getenv("ZOTERO_API_KEY")
        user_id = os.getenv("ZOTERO_USER_ID")
        
        if not api_key or not user_id:
            return None
        
        try:
            headers = {"Zotero-API-Key": api_key}
            response = requests.get(
                f"https://api.zotero.org/users/{user_id}/collections",
                headers=headers,
                params={"limit": 100}
            )
            
            if response.status_code != 200:
                return None
            
            collections = response.json()
            for collection in collections:
                if collection['data']['name'] == collection_name:
                    return collection['data']['key']
            
            print(f"âš ï¸  Collection '{collection_name}' not found in Zotero library", flush=True)
            return None
            
        except Exception as e:
            print(f"âŒ Error looking up collection: {e}", flush=True)
            return None
    
    @staticmethod
    def export_to_zotero(markdown_path: Path, source_file: str,
                        qwen_model: str, pages_processed: int,
                        title: Optional[str] = None,
                        collection_key: Optional[str] = None) -> Optional[str]:
        """
        Export processed text to Zotero library with metadata and markdown attachment
        
        Returns:
            Zotero item key on success, None on failure
        """
        api_key = os.getenv("ZOTERO_API_KEY")
        user_id = os.getenv("ZOTERO_USER_ID")
        
        if not api_key or not user_id:
            print("âš ï¸  Warning: ZOTERO_API_KEY or ZOTERO_USER_ID not set. Skipping Zotero export.", flush=True)
            return None
        
        try:
            # Prepare metadata
            source_name = Path(source_file).stem
            display_title = title if title else source_name
            process_date = datetime.now().strftime('%Y-%m-%d')
            
            # Create Zotero item
            item_data = {
                "itemType": "book",
                "title": display_title,
                "abstractNote": f"OCR-processed classical Chinese text from {Path(source_file).name}",
                "date": process_date,
                "language": "zh",
                "extra": (
                    f"OCR Engine: Qwen-VL ({qwen_model})\n"
                    f"Source File: {Path(source_file).name}\n"
                    f"Pages Processed: {pages_processed}\n"
                    f"Processing Date: {process_date}"
                )
            }
            
            # Add to collection if specified
            if collection_key:
                item_data["collections"] = [collection_key]
            
            # Create item in Zotero
            headers = {
                "Zotero-API-Key": api_key,
                "Content-Type": "application/json"
            }
            
            response = requests.post(
                f"https://api.zotero.org/users/{user_id}/items",
                headers=headers,
                json=[item_data]
            )
            
            if response.status_code != 200:
                print(f"âŒ Failed to create Zotero item: {response.status_code}", flush=True)
                print(response.text, flush=True)
                return None
            
            # Get the created item key
            item_key = response.json()['successful']['0']['key']
            print(f"âœ“ Created Zotero item: {item_key}", flush=True)
            
            # Attach markdown file
            with open(markdown_path, 'rb') as f:
                file_content = f.read()
            
            attachment_response = requests.post(
                f"https://api.zotero.org/users/{user_id}/items/{item_key}/file",
                headers={
                    "Zotero-API-Key": api_key,
                    "Content-Type": "text/markdown",
                    "If-None-Match": "*"
                },
                data=file_content
            )
            
            if attachment_response.status_code == 200:
                print(f"âœ“ Attached markdown file to Zotero item", flush=True)
            else:
                print(f"âš ï¸  Warning: Failed to attach file: {attachment_response.status_code}", flush=True)
            
            return item_key
            
        except Exception as e:
            print(f"âŒ Error exporting to Zotero: {e}", flush=True)
            import traceback
            traceback.print_exc()
            return None

# ============================================================================
# OCR ENGINE
# ============================================================================

class OCREngine:
    """Handles Qwen-VL OCR operations"""

    def __init__(self, clients: APIClients, config: ProcessingConfig):
        self.clients = clients
        self.config = config
        self.text_processor = TextProcessor()

    def process_image(self, image_path: str, page_num: int, total_pages: int, current_index: int = None) -> Optional[Tuple[str, bool]]:
        """Process a single image and return text with loop detection flag"""
        # Use current_index for progress calculation if provided, otherwise fall back to page_num
        progress_index = current_index if current_index is not None else page_num
        progress = f"({progress_index}/{total_pages}, {progress_index*100//total_pages}%)" if total_pages > 1 else ""
        print(f"\n--- Page {page_num} {progress} ---", flush=True)

        # Preprocess image
        processed_path = ImageProcessor.preprocess_image(image_path, self.config)
        temp_preprocessed = processed_path if processed_path != image_path else None

        try:
            # Read and encode image as base64
            with open(processed_path, 'rb') as image_file:
                image_content = base64.b64encode(image_file.read()).decode('utf-8')

            # API call with retry logic
            text = None
            for attempt in range(self.config.retry_attempts):
                try:
                    response = self.clients.qwen.chat.completions.create(
                        model=self.config.qwen_model,
                        messages=[{
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": (
                                       "ä½ æ˜¯ä¸€ä¸ªå¤å…¸æ–‡çŒ®OCRä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹è¦æ±‚æå–å›¾ç‰‡ä¸­çš„å…¨éƒ¨ä¸­æ–‡æ–‡å­—:\n"
                                        "1. ç‰ˆé¢æ ¼å¼:ä¼ ç»Ÿç«–æ’ç‰ˆå¼,ä»å³åˆ°å·¦ã€ä»ä¸Šåˆ°ä¸‹é˜…è¯»\n"
                                        "2. æ¯åˆ—çº¦1-25å­—,å®Œæ•´æå–æ¯ä¸€åˆ—\n"
                                        "3. ä¿ç•™æ‰€æœ‰æ ‡ç‚¹ã€ç©ºæ ¼å’Œæ¢è¡Œ\n"
                                        "4. å¿½ç•¥é¡µç ã€å°ç« ã€æ°´å°ã€å›¾ä¹¦é¦†æ ‡è®°ç­‰å…ƒæ•°æ®ã€‚è‹¥é¡µé¢æ–‡å­—å†…å®¹å°‘äº10ä¸ªå­—ç¬¦ï¼Œæˆ–ä¸»è¦åŒºåŸŸä¸ºå›¾ç”»/å›¾è¡¨ï¼Œè¾“å‡º'[å›¾]ã€‚è‹¥é¡µé¢å®Œå…¨æ— æ–‡å­—å†…å®¹ï¼Œæˆ–ä»…æœ‰ä»…æœ‰é¡µç /å›¾ä¹¦é¦†æ ‡è®°ï¼Œè¾“å‡º'[ç©ºé¡µ]'\n"
                                        "5. å¦‚æœé‡åˆ°æ¨¡ç³Šæˆ–ç ´æŸæ–‡å­—,ç”¨ã€?ã€‘æ ‡æ³¨\n"
                                        "6. åªæå–é¡µé¢ä¸»ä½“æ–‡å­—åŒºåŸŸ,å¿½ç•¥è£…è®¢è¾¹å’Œé¡µè¾¹è·\n"
                                        "7. è¾“å‡ºçº¯æ–‡æœ¬,ä¸è¦æ·»åŠ ä»»ä½•è¯´æ˜æˆ–æ ‡é¢˜\n"
                                        "8. ç‰¹æ®Šæ ¼å¼å¤„ç†ï¼š å¦‚é‡è¡¨æ ¼ã€åå†Œã€è´¦ç›®ç­‰è¡Œåˆ—å¯¹é½çš„ç‰ˆå¼ï¼Œè¯·ä¼˜å…ˆä¿æŒå…¶è¡Œåˆ—ç»“æ„ã€‚å¯ä½¿ç”¨æ¢è¡Œå’Œç©ºæ ¼æ¥åŒºåˆ†ä¸åŒæ¡ç›®ï¼Œç¡®ä¿åŒä¸€è¡Œçš„æ•°æ®ä¿æŒåœ¨åŒä¸€è¡Œ\n"
                                        "9. ä¸¥æ ¼ä¿æŒåŸæ–‡ç”¨å­—ï¼šå¦‚åŸæ–‡ä¸ºç¹ä½“å­—ï¼Œè¾“å‡ºä¸€å¾‹ä½¿ç”¨ç¹ä½“ï¼›ä»…å½“åŸæ–‡ç¡®ä¸ºç®€åŒ–å­—æ—¶æ–¹ç”¨ç®€ä½“"
                                        "\nè¯·å¼€å§‹æå–:"
                                    )
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {"url": f"data:image/png;base64,{image_content}"}
                                }
                            ]
                        }],
                        max_tokens=4096,
                        temperature=0.1,
                        timeout=self.config.api_timeout
                    )
                    
                    text = response.choices[0].message.content.strip()
                    break  # Success, exit retry loop
                    
                except Exception as e:
                    if attempt < self.config.retry_attempts - 1:
                        wait_time = self.config.retry_delay * (2 ** attempt)
                        print(f"  âš ï¸  OCR error (attempt {attempt+1}/{self.config.retry_attempts}): {e}", flush=True)
                        print(f"  Retrying in {wait_time}s...", flush=True)
                        time.sleep(wait_time)
                    else:
                        print(f"  âŒ All OCR retry attempts failed: {e}", flush=True)
                        return None

            if text is None:
                return None

            char_count = len(text)

            # Progressive quality checks with our agreed thresholds
            if char_count > self.config.expected_chars_normal:
                print(f"  âš ï¸  Long page: {char_count} chars (expected <{self.config.expected_chars_normal})", flush=True)

            if char_count > self.config.ocr_soft_cap:
                print(f"  âš ï¸  Very long page: {char_count} chars (soft cap: {self.config.ocr_soft_cap})", flush=True)

            if char_count > self.config.ocr_hard_cap:
                print(f"  ğŸ”´ Excessive length: {char_count} chars - truncating to {self.config.ocr_hard_cap}", flush=True)
                text = text[:self.config.ocr_hard_cap]

            # Detect loops and first pass truncation
            has_loops = self.text_processor.detect_ocr_loops_simple(text)

            if has_loops:
                print(f"  ğŸ”„ Potential OCR loops detected - truncating aggressively", flush=True)
                # Find first major repetition and cut there
                text = ImageProcessor.truncate_at_loop(text, max_length=800)

            print(f"  âœ“ OCR success ({len(text)} chars{', has loops' if has_loops else ''})", flush=True)

            # Remove library metadata
            cleaned_text = self.text_processor.remove_metadata_text(text)

            return cleaned_text, has_loops

        finally:
            # Clean up temporary preprocessed image
            if temp_preprocessed and os.path.exists(temp_preprocessed):
                try:
                    os.unlink(temp_preprocessed)
                except:
                    pass

# ============================================================================
# TEXT CLEANING ENGINE
# ============================================================================

class TextCleaner:
    """Handles text cleaning operations with Kimi or DeepSeek"""

    def __init__(self, clients: APIClients, config: ProcessingConfig):
        self.clients = clients
        self.config = config
        self.text_processor = TextProcessor()

    def _get_cleanup_client_and_model(self):
        """Get the appropriate client and model name for cleanup operations"""
        if self.config.cleanup_model == "deepseek":
            return self.clients.deepseek, "deepseek-chat"
        else:  # default to kimi
            return self.clients.kimi, "kimi-k2-0905-preview"

    def sanitize_output(self, text: str) -> str:
        """
        Remove context markers if they accidentally appear in output.
        This is a failsafe - should rarely be needed if prompts work correctly.
        """
        # Remove any [[...]] blocks that slipped through
        cleaned = re.sub(r'\[\[ä¸Šæ–‡å‚è€ƒ[^\]]*\]\]', '', text, flags=re.DOTALL)
        cleaned = cleaned.strip()
        return cleaned

    def detect_overlap_duplicate(self, prev_tail: str, curr_head: str,
                                 threshold: float = 0.85, min_length: int = 200) -> int:
        """
        Detect if curr_head contains a duplicate of prev_tail.
        Returns: length of duplicate if found, 0 otherwise.

        Uses character-level matching with high threshold to be conservative.
        """
        # Search for matching sequences at the boundary
        max_search = min(len(prev_tail), len(curr_head), 800)
        best_match_len = 0

        # Look for matches starting from the end of prev_tail
        for i in range(max_search, min_length, -10):  # Step by 10 for efficiency
            tail_segment = prev_tail[-i:]
            if curr_head.startswith(tail_segment):
                # Found exact match at boundary
                if i >= min_length:
                    similarity = 1.0
                    if similarity >= threshold:
                        best_match_len = i
                        break
            else:
                # Check fuzzy match
                matches = 0
                check_len = min(i, len(curr_head))
                for j in range(check_len):
                    if j < len(prev_tail) and prev_tail[-(i-j)] == curr_head[j]:
                        matches += 1

                similarity = matches / i if i > 0 else 0
                if similarity >= threshold and i >= min_length:
                    best_match_len = i
                    break

        return best_match_len

    def clean_chunk(self, text_chunk: str, context: str = "", has_ocr_loops: bool = False,
                   has_marked_context: bool = False) -> str:
        """Clean a single chunk of text with retry logic"""
        start_time = time.time()
        loop_warning = ""
        if has_ocr_loops:
            loop_warning = "\n\n**é‡è¦æç¤º**ï¼šæ­¤æ–‡æœ¬å¯èƒ½åŒ…å«OCRè¯†åˆ«å¾ªç¯å¯¼è‡´çš„é‡å¤å†…å®¹ï¼Œè¯·ä»”ç»†æ£€æŸ¥å¹¶åˆ é™¤æ‰€æœ‰é‡å¤éƒ¨åˆ†ã€‚"

        # Base requirements
        base_requirements = """1. ç»å¯¹å¿…é¡»ä¿æŒåŸæ–‡çš„ç¹ç®€ä½“å­—å½¢å¼ã€‚ä¸¥ç¦æ“…è‡ªè½¬æ¢å­—å½¢ã€‚
2. ã€æ ¸å¿ƒåŸåˆ™ã€‘ä¸¥æ ¼ä¿æŒæ‰€æœ‰åŸæ–‡å­—ç¬¦ä¸å˜ã€‚**ä»…åœ¨ä»¥ä¸‹ä¸‰ç§æƒ…å†µå¯è€ƒè™‘ä¿®æ­£**ï¼š
   a. **å­—å½¢é«˜åº¦ç›¸ä¼¼ä¸”è¯­å¢ƒå®Œå…¨ä¸é€š**ï¼ˆå¦‚ã€Œå·±ã€ã€Œå·²ã€ã€Œå·³ã€åœ¨æ˜æ˜¾é”™è¯¯çš„è¯­å¢ƒï¼‰
   b. **æ˜æ˜¾ä¸ç¬¦åˆæ—¶ä»£ç‰¹å¾çš„ç”¨å­—**ï¼ˆå¦‚ç°ä»£ç®€ä½“å­—æ··å…¥æ˜æ¸…æ–‡çŒ®ï¼‰
   c. **åŒä¸€å­—åœ¨æ–‡ä¸­ç¨³å®šå‡ºç°ï¼Œä»…ä¸ªåˆ«å¤„æ˜æ˜¾è¯¯å†™**ï¼ˆå‚è€ƒä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼‰
   ä¿®æ­£æ—¶å¿…é¡»é€‰æ‹©**æœ€æ¥è¿‘åŸå­—å½¢**çš„åˆç†æ±‰å­—ï¼Œå¹¶åœ¨ä¿®æ­£å¤„æ·»åŠ ã€ï¼Ÿã€‘æ ‡æ³¨ã€‚
3. æ·»åŠ æ–‡è¨€æ–‡é€‚ç”¨çš„æ ‡ç‚¹ç¬¦å·ï¼ˆå¥å·ã€é€—å·ã€é¡¿å·ã€é—®å·ï¼‰ã€‚
4. åˆ é™¤æ‰€æœ‰æœºæ„å…ƒæ•°æ®ï¼ˆå¦‚é¡µç ã€å›¾ä¹¦é¦†æ ‡è®°ã€[ç©ºé¡µ]ç­‰ï¼‰ã€‚
5. å°†æ–‡æœ¬æ•´ç†æˆè¿è´¯æ®µè½ï¼Œä½†**ä¿æŒåŸæ–‡çš„ç« èŠ‚å±‚æ¬¡å’Œè‡ªç„¶æ¢è¡Œ**ã€‚
6. å¯¹äºæ— æ³•ç¡®å®šçš„å­—ï¼Œç”¨ã€ï¼Ÿã€‘æ ‡æ³¨ã€‚
7. **ç‰¹åˆ«æ³¨æ„ï¼šå¦‚æœå‘ç°é‡å¤å‡ºç°çš„æ–‡æœ¬å—ï¼ˆOCRè¯†åˆ«å¾ªç¯é”™è¯¯ï¼‰ï¼Œè¯·åˆ é™¤è¿™äº›é‡å¤éƒ¨åˆ†**ã€‚
8. **å¦‚æœæ–‡æœ¬æ˜æ˜¾ä¸å®Œæ•´æˆ–è¢«æˆªæ–­ï¼Œä¿æŒåŸæ ·å³å¯**ã€‚
9. ã€æ–°å¢ã€‘**å¯¹äºå¤æ±‰è¯­ä¸­çš„å¼‚ä½“å­—ã€é€šå‡å­—ã€é¿è®³å­—ç­‰ï¼Œå³ä½¿çœ‹èµ·æ¥ä¸å¸¸è§ï¼Œä¹Ÿå¿…é¡»ä¿ç•™åŸå­—**ã€‚

é‡è¦æé†’ï¼šå½“ä¸ç¡®å®šæ˜¯å¦åº”è¯¥ä¿®æ­£æ—¶ï¼Œä¸€å¾‹é€‰æ‹©**ä¿ç•™åŸå­—**ã€‚"""

        # Add special instructions for chunks with marked context
        if has_marked_context:
            context_instruction = """

**é‡è¦è¯´æ˜ - å…³äºä¸Šæ–‡å‚è€ƒ**ï¼š
- æ–‡æœ¬å¼€å¤´æœ‰ [[ä¸Šæ–‡å‚è€ƒ...]] æ‹¬èµ·çš„éƒ¨åˆ†ï¼Œè¿™æ˜¯å‰ä¸€æ®µçš„ç»“å°¾ï¼Œå·²ç»å¤„ç†è¿‡
- [[...]] ä¸­çš„å†…å®¹ä»…ä¾›ä½ ç†è§£ä¸Šä¸‹æ–‡ï¼Œå¸®åŠ©ä½ æ­£ç¡®ç†è§£æ¥ä¸‹æ¥çš„æ–‡æœ¬
- **ç»å¯¹ä¸è¦åœ¨è¾“å‡ºä¸­åŒ…å«æˆ–é‡å¤ [[...]] ä¸­çš„ä»»ä½•å†…å®¹**
- åªå¤„ç†å¹¶è¾“å‡º [[...]] ä¹‹åçš„æ–‡æœ¬
- å¦‚æœ [[...]] åçš„æ–‡æœ¬å¼€å¤´æ˜¯ä¸å®Œæ•´çš„å¥å­ï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡ç†è§£å…¶å«ä¹‰ï¼Œä½†ä»ç„¶ä¸è¦è¾“å‡ºä¸Šæ–‡å‚è€ƒçš„å†…å®¹
- ç›´æ¥ä» [[...]] åé¢çš„ç¬¬ä¸€ä¸ªå­—å¼€å§‹è¾“å‡ºä½ çš„å¤„ç†ç»“æœ"""

            prompt = f"""è¯·å°†ä»¥ä¸‹OCRæ–‡æœ¬æ•´ç†æˆè¿è´¯çš„æ–‡è¨€æ–‡æ–‡çŒ®ã€‚é€™æ˜¯æ˜æ¸…æ™‚æœŸçš„æ–‡ç»ã€‚

è¦æ±‚ï¼š
{base_requirements}{loop_warning}{context_instruction}

{f"ä¸Šä¸‹æ–‡æç¤ºï¼š{context}" if context else ""}

å¾…å¤„ç†æ–‡æœ¬ï¼š
{text_chunk}

è¯·ç›´æ¥è¿”å›æ•´ç†åçš„æ–‡æœ¬ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è¯´æ˜ã€åˆ—è¡¨æˆ–æ ‡é¢˜ï¼Œç«‹å³å¼€å§‹æ­£æ–‡ï¼š"""
        else:
            prompt = f"""è¯·å°†ä»¥ä¸‹OCRæ–‡æœ¬æ•´ç†æˆè¿è´¯çš„æ–‡è¨€æ–‡æ–‡çŒ®ã€‚é€™æ˜¯æ˜æ¸…æ™‚æœŸçš„æ–‡ç»ã€‚

è¦æ±‚ï¼š
{base_requirements}{loop_warning}

{f"ä¸Šä¸‹æ–‡æç¤ºï¼š{context}" if context else ""}

OCRæ–‡æœ¬ï¼š
{text_chunk}

è¯·ç›´æ¥è¿”å›æ•´ç†åçš„æ–‡æœ¬ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è¯´æ˜ã€åˆ—è¡¨æˆ–æ ‡é¢˜ï¼Œç«‹å³å¼€å§‹æ­£æ–‡ï¼š"""

        for attempt in range(self.config.retry_attempts):
            try:
                sys.stdout.flush()  # Force flush before API call
                client, model = self._get_cleanup_client_and_model()
                response = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    timeout=self.config.api_timeout,
                    max_tokens=2048
                )
                processing_time = time.time() - start_time
                print(f"âœ“ ({processing_time:.1f}s)", flush=True)

                # Sanitize output to remove any context markers that slipped through
                cleaned_output = self.sanitize_output(response.choices[0].message.content)
                return cleaned_output

            except Exception as e:
                print(f"\nâœ— Error (attempt {attempt+1}/{self.config.retry_attempts}): {e}", flush=True)
                if attempt < self.config.retry_attempts - 1:
                    wait_time = self.config.retry_delay * (2 ** attempt)
                    print(f"    Retrying in {wait_time} seconds...", flush=True)
                    time.sleep(wait_time)
                else:
                    print("    All retry attempts failed. Returning original text.", flush=True)
                    return text_chunk

        return text_chunk

    def create_chunks_with_overlap(self, text: str) -> List[Tuple[int, int]]:
        """Create overlapping chunks for processing (stores indices only, not text)"""
        # Validate configuration to prevent infinite loops and data loss
        if self.config.max_text_length <= 0:
            raise ValueError(
                f"max_text_length ({self.config.max_text_length}) must be positive"
            )
        if self.config.chunk_overlap < 0:
            raise ValueError(
                f"chunk_overlap ({self.config.chunk_overlap}) must be non-negative"
            )
        if self.config.chunk_overlap >= self.config.max_text_length:
            raise ValueError(
                f"chunk_overlap ({self.config.chunk_overlap}) must be less than "
                f"max_text_length ({self.config.max_text_length}) to avoid infinite loops"
            )

        chunks = []
        start = 0

        while start < len(text):
            end = min(start + self.config.max_text_length, len(text))
            chunks.append((start, end))  # Store indices only, not chunk text

            # If we've reached the end of the text, stop
            if end >= len(text):
                break

            start = end - self.config.chunk_overlap

        print(f"Created {len(chunks)} chunks with {self.config.chunk_overlap}-char overlap", flush=True)
        return chunks

    def process_text_in_chunks(self, text: str, context: str = "", has_ocr_loops: bool = False) -> str:
        """
        Process text in chunks with marked context to prevent duplication.

        Strategy:
        1. First chunk: process normally
        2. Subsequent chunks: add [[ä¸Šæ–‡å‚è€ƒ...]] marker with previous overlap
        3. Apply conservative deduplication as safety net
        """
        if len(text) <= self.config.max_text_length:
            print("Text fits in single chunk, processing...", flush=True)
            return self.clean_chunk(text, context, has_ocr_loops, has_marked_context=False)

        print(f"Text length: {len(text)} chars - splitting into chunks...", flush=True)
        chunk_indices = self.create_chunks_with_overlap(text)
        cleaned_chunks = []

        # Process chunks one at a time
        for i, (start, end) in enumerate(chunk_indices, 1):
            print(f"\nProcessing chunk {i}/{len(chunk_indices)} (chars {start}-{end})... ", end='', flush=True)

            # Extract chunk text from original
            chunk_text = text[start:end]

            # For non-first chunks, add context marker
            if i > 1:
                # Get context from original text (previous overlap region)
                context_start = max(0, start - self.config.chunk_overlap)
                context_text = text[context_start:start]

                # Create marked chunk with explicit context
                marked_chunk = f"""[[ä¸Šæ–‡å‚è€ƒï¼ˆå·²å¤„ç†å®Œæ¯•ï¼Œè¯·å‹¿åœ¨è¾“å‡ºä¸­é‡å¤ï¼‰ï¼š
{context_text}
]]

{chunk_text}"""

                print(f"With marked context, sending to Kimi API... ", end='', flush=True)
                cleaned = self.clean_chunk(marked_chunk, context, has_ocr_loops, has_marked_context=True)
            else:
                print(f"First chunk, sending to Kimi API... ", end='', flush=True)
                cleaned = self.clean_chunk(chunk_text, context, has_ocr_loops, has_marked_context=False)

            cleaned_chunks.append(cleaned)
            del chunk_text  # Explicitly release references

        # Clear chunk indices
        del chunk_indices

        # SAFETY NET: Apply conservative deduplication between chunks
        print(f"\n\nApplying deduplication safety net...", flush=True)
        deduped_chunks = [cleaned_chunks[0]]  # First chunk always kept as-is

        for i in range(1, len(cleaned_chunks)):
            prev_chunk = deduped_chunks[-1]
            curr_chunk = cleaned_chunks[i]

            # Check for duplicates at the boundary
            duplicate_len = self.detect_overlap_duplicate(
                prev_tail=prev_chunk[-800:],  # Check last 800 chars of previous
                curr_head=curr_chunk[:800],    # Check first 800 chars of current
                threshold=0.85,
                min_length=200
            )

            if duplicate_len > 0:
                print(f"   Chunk {i+1}: Removed {duplicate_len}-char duplicate", flush=True)
                deduped_chunks.append(curr_chunk[duplicate_len:])
            else:
                deduped_chunks.append(curr_chunk)

        # Join deduplicated chunks
        combined = '\n\n'.join(deduped_chunks)
        print(f"âœ“ Combined {len(deduped_chunks)} chunks into {len(combined)} chars", flush=True)

        # Clear intermediate lists
        del cleaned_chunks
        del deduped_chunks

        return combined

    def generate_review_report(self, text: str, context: str = "") -> str:
        """
        Generate quality review report for processed text.
        DOES NOT rewrite text - only identifies issues for manual review.
        Prevents data loss from token limits.
        """
        print("\n=== Quality Review Analysis ===", flush=True)

        # Sample text for analysis (first 20000 chars + last 20000 chars)
        # This ensures we check beginning and end without exceeding token limits
        text_length = len(text)
        if text_length > 40000:
            sample_text = text[:20000] + "\n\n[...ä¸­é—´éƒ¨åˆ†å·²çœç•¥...]\n\n" + text[-20000:]
            print(f"   Analyzing sample: {text_length} chars (sampling first/last 20k)", flush=True)
        else:
            sample_text = text
            print(f"   Analyzing full text: {text_length} chars", flush=True)

        prompt = f"""ä½ æ˜¯ä¸€ä½å¤ç±æ•´ç†ä¸“å®¶ã€‚è¯·ä»”ç»†å®¡é˜…ä»¥ä¸‹å·²å¤„ç†çš„æ–‡è¨€æ–‡æ–‡æœ¬ï¼Œè¯†åˆ«å¯èƒ½å­˜åœ¨çš„é—®é¢˜ã€‚

è¿™æ˜¯æ˜æ¸…æ—¶æœŸçš„æ–‡çŒ®ã€‚æ–‡æœ¬å·²ç»è¿‡OCRè¯†åˆ«å’Œåˆæ­¥æ•´ç†ã€‚

è¯·åˆ†ææ–‡æœ¬å¹¶åˆ—å‡ºä»¥ä¸‹ç±»å‹çš„é—®é¢˜ï¼š

1. **ç¹ç®€ä½“æ··ç”¨**ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ“…è‡ªå°†ç¹é«”å­—è½¬æ¢ä¸ºç®€ä½“å­—ï¼ˆæˆ–åä¹‹ï¼‰çš„æƒ…å†µï¼Œè¿™æ˜¯ä¸¥é‡é”™è¯¯
2. **é‡å¤å†…å®¹**ï¼šæ˜æ˜¾é‡å¤çš„æ®µè½æˆ–å¥å­ï¼ˆéåŸæ–‡æœ¬èº«çš„é‡å¤ç»“æ„ï¼‰
3. **OCRé”™è¯¯**ï¼šå¯èƒ½çš„è¯¯è®¤å­—ï¼ˆå½¢è¿‘å­—æ··æ·†ï¼‰
4. **æ ‡ç‚¹é—®é¢˜**ï¼šæ ‡ç‚¹ç¬¦å·ä½¿ç”¨æ˜æ˜¾ä¸å½“æˆ–ä¸ä¸€è‡´
5. **æ ¼å¼é—®é¢˜**ï¼šæ®µè½ç»“æ„æ··ä¹±æˆ–å¼‚å¸¸
6. **è¯´æ˜æ€§æ–‡å­—**ï¼šAIå¯èƒ½æ·»åŠ çš„æ³¨é‡Šæˆ–è¯´æ˜ï¼ˆåº”åˆ é™¤ï¼‰
7. **æˆªæ–­é—®é¢˜**ï¼šæ–‡æœ¬å¼€å¤´æˆ–ç»“å°¾æ˜¯å¦æœ‰æˆªæ–­è¿¹è±¡

{f"ä¸Šä¸‹æ–‡æç¤ºï¼š{context}" if context else ""}

æ–‡æœ¬æ ·æœ¬ï¼š
{sample_text}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºå®¡æŸ¥æŠ¥å‘Šï¼š

## è´¨é‡å®¡æŸ¥æŠ¥å‘Š

### 1. é‡å¤å†…å®¹
[å¦‚æœ‰å‘ç°ï¼Œåˆ—å‡ºå…·ä½“ä½ç½®å’Œå†…å®¹ï¼›å¦‚æ— ï¼Œå†™"æœªå‘ç°"]

### 2. å¯èƒ½çš„OCRé”™è¯¯
[åˆ—å‡ºå¯ç–‘çš„å­—ç¬¦ï¼Œå¦‚"æŸæŸæŸï¼ˆç–‘ä¸ºXXï¼‰"ï¼›å¦‚æ— ï¼Œå†™"æœªå‘ç°æ˜æ˜¾é”™è¯¯"]

### 3. æ ‡ç‚¹é—®é¢˜
[åˆ—å‡ºæ ‡ç‚¹ä½¿ç”¨ä¸å½“çš„åœ°æ–¹ï¼›å¦‚æ— ï¼Œå†™"æ ‡ç‚¹ä½¿ç”¨åŸºæœ¬æ­£å¸¸"]

### 4. æ ¼å¼é—®é¢˜
[åˆ—å‡ºæ ¼å¼æ··ä¹±çš„åœ°æ–¹ï¼›å¦‚æ— ï¼Œå†™"æ ¼å¼åŸºæœ¬æ­£å¸¸"]

### 5. è¯´æ˜æ€§æ–‡å­—
[åˆ—å‡ºéœ€è¦åˆ é™¤çš„æ³¨é‡Šæˆ–è¯´æ˜ï¼›å¦‚æ— ï¼Œå†™"æœªå‘ç°"]

### 6. æ–‡æœ¬å®Œæ•´æ€§
[è¯„ä¼°æ–‡æœ¬å¼€å¤´å’Œç»“å°¾æ˜¯å¦å®Œæ•´ï¼›æ˜¯å¦æœ‰æˆªæ–­]

### 7. æ€»ä½“è¯„ä»·
[ç®€è¦è¯„ä»·å¤„ç†è´¨é‡ï¼Œ1-5æ˜Ÿ]

### 8. å»ºè®®
[ç»™å‡ºäººå·¥å®¡æ ¡çš„é‡ç‚¹å»ºè®®]

è¯·å®¢è§‚ã€å…·ä½“åœ°æŒ‡å‡ºé—®é¢˜ï¼Œä¾¿äºäººå·¥å®¡æ ¡ã€‚"""

        for attempt in range(self.config.retry_attempts):
            try:
                client, model = self._get_cleanup_client_and_model()
                response = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    timeout=self.config.api_timeout,
                    max_tokens=4096  # Sufficient for review report
                )
                review_report = response.choices[0].message.content
                print("âœ“ Quality review complete", flush=True)
                return review_report

            except Exception as e:
                print(f"âœ— Review error (attempt {attempt+1}/{self.config.retry_attempts}): {e}", flush=True)
                if attempt < self.config.retry_attempts - 1:
                    wait_time = self.config.retry_delay * (2 ** attempt)
                    print(f"    Retrying in {wait_time} seconds...", flush=True)
                    time.sleep(wait_time)
                else:
                    print("    All retry attempts failed. Skipping review.", flush=True)
                    return "è´¨é‡å®¡æŸ¥å¤±è´¥ï¼šAPIè°ƒç”¨è¶…æ—¶æˆ–å‡ºé”™\n\nå»ºè®®äººå·¥å…¨é¢å®¡æ ¡æ–‡æœ¬ã€‚"

        return "è´¨é‡å®¡æŸ¥å¤±è´¥ï¼šè¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°\n\nå»ºè®®äººå·¥å…¨é¢å®¡æ ¡æ–‡æœ¬ã€‚"

class HighConcurrencyTextCleaner(TextCleaner):
    """
    Text cleaner optimized for high-concurrency APIs (Kimi Tier 3, DeepSeek unlimited).
    Dynamically adjusts concurrency based on service and document size.
    """

    # Service-specific limits (conservative defaults)
    SERVICE_LIMITS = {
        "kimi": {
            "max_concurrent": 50,     # Conservative: 50 of 200 available
            "max_tokens_per_minute": 2500000,  # 2.5M of 3M TPM
            "requests_per_minute": 600,        # 10 RPS
            "tokens_per_request": 3500,        # Average estimate
        },
        "deepseek": {
            "max_concurrent": 100,    # No official limit, be reasonable
            "max_tokens_per_minute": float('inf'),  # No limit claimed
            "requests_per_minute": float('inf'),     # No limit claimed
            "tokens_per_request": 3500,
        }
    }

    def __init__(self, clients: APIClients, config: ProcessingConfig):
        super().__init__(clients, config)
        self.service = config.cleanup_model
        self.limits = self.SERVICE_LIMITS[self.service]

        # Rate limiting state
        self.token_counter = 0
        self.request_counter = 0
        self.window_start = time.time()
        self.lock = threading.Lock()

        # Performance tracking
        self.response_times = []
        self.success_count = 0
        self.failure_count = 0

    def process_text_in_chunks(self, text: str, context: str = "",
                              has_ocr_loops: bool = False) -> str:
        """
        Main entry point with intelligent parallelization.
        """
        chunk_indices = self.create_chunks_with_overlap(text)
        total_chunks = len(chunk_indices)

        # Decision logic for parallelization
        if total_chunks <= 3:
            print(f"ğŸ“ Small document ({total_chunks} chunks), using sequential mode")
            return super().process_text_in_chunks(text, context, has_ocr_loops)

        # Calculate optimal concurrency
        optimal_workers = self._calculate_optimal_concurrency(total_chunks)

        print(f"ğŸš€ Processing {total_chunks} chunks with {optimal_workers} parallel workers "
              f"({self.service.upper()})")

        return self._process_high_concurrency(
            text, chunk_indices, context, has_ocr_loops, optimal_workers
        )

    def _calculate_optimal_concurrency(self, total_chunks: int) -> int:
        """Calculate optimal number of workers."""
        max_concurrent = self.limits["max_concurrent"]

        if total_chunks <= 10:
            return min(5, total_chunks, max_concurrent)

        if total_chunks <= 50:
            return min(total_chunks // 2, max_concurrent)

        optimal = min(total_chunks, max_concurrent)

        if len(self.response_times) > 10:
            avg_time = sum(self.response_times[-10:]) / 10
            if avg_time > 30:
                optimal = max(10, optimal // 2)
                print(f"   âš¡ Reducing concurrency to {optimal} (slow responses)")

        return optimal

    def _process_high_concurrency(self, text: str,
                                 chunk_indices: List[Tuple[int, int]],
                                 context: str, has_ocr_loops: bool,
                                 max_workers: int) -> str:
        """High-concurrency parallel processing."""
        total_chunks = len(chunk_indices)
        start_time = time.time()

        # Prepare all tasks
        tasks = []
        for i, (start, end) in enumerate(chunk_indices, 1):
            chunk_text = text[start:end]
            is_first_chunk = (i == 1)

            prompt = self._build_chunk_prompt(chunk_text, context, has_ocr_loops, not is_first_chunk)

            tasks.append({
                'index': i-1,
                'chunk_text': chunk_text,
                'prompt': prompt,
                'is_first_chunk': is_first_chunk,
                'estimated_tokens': len(chunk_text) * 1.5 + 500
            })

        # Process in parallel
        results = self._execute_with_adaptive_concurrency(tasks, max_workers)

        # Combine results
        combined_text = self._combine_results(results, total_chunks)

        elapsed = time.time() - start_time
        self._print_performance_summary(elapsed, total_chunks, len(combined_text))

        return combined_text

    def _build_chunk_prompt(self, chunk_text: str, context: str,
                           has_loops: bool, has_marked_context: bool) -> str:
        """Build prompt for a single chunk."""
        # Use the prompt building logic from your existing clean_chunk method
        # Extract and adapt it here (or call super().clean_chunk with a flag)
        loop_warning = "\n\n**é‡è¦æç¤º**ï¼šæ­¤æ–‡æœ¬å¯èƒ½åŒ…å«OCRè¯†åˆ«å¾ªç¯å¯¼è‡´çš„é‡å¤å†…å®¹ï¼Œè¯·ä»”ç»†æ£€æŸ¥å¹¶åˆ é™¤æ‰€æœ‰é‡å¤éƒ¨åˆ†ã€‚" if has_loops else ""

        base_requirements = """1. ç»å¯¹å¿…é¡»ä¿æŒåŸæ–‡çš„ç¹ç®€ä½“å­—å½¢å¼ã€‚ä¸¥ç¦æ“…è‡ªè½¬æ¢å­—å½¢ã€‚
2. ã€æ ¸å¿ƒåŸåˆ™ã€‘ä¸¥æ ¼ä¿æŒæ‰€æœ‰åŸæ–‡å­—ç¬¦ä¸å˜ã€‚**ä»…åœ¨ä»¥ä¸‹ä¸‰ç§æƒ…å†µå¯è€ƒè™‘ä¿®æ­£**ï¼š
   a. **å­—å½¢é«˜åº¦ç›¸ä¼¼ä¸”è¯­å¢ƒå®Œå…¨ä¸é€š**ï¼ˆå¦‚ã€Œå·±ã€ã€Œå·²ã€ã€Œå·³ã€åœ¨æ˜æ˜¾é”™è¯¯çš„è¯­å¢ƒï¼‰
   b. **æ˜æ˜¾ä¸ç¬¦åˆæ—¶ä»£ç‰¹å¾çš„ç”¨å­—**ï¼ˆå¦‚ç°ä»£ç®€ä½“å­—æ··å…¥æ˜æ¸…æ–‡çŒ®ï¼‰
   c. **åŒä¸€å­—åœ¨æ–‡ä¸­ç¨³å®šå‡ºç°ï¼Œä»…ä¸ªåˆ«å¤„æ˜æ˜¾è¯¯å†™**ï¼ˆå‚è€ƒä¸Šä¸‹æ–‡ä¸€è‡´æ€§ï¼‰
   ä¿®æ­£æ—¶å¿…é¡»é€‰æ‹©**æœ€æ¥è¿‘åŸå­—å½¢**çš„åˆç†æ±‰å­—ï¼Œå¹¶åœ¨ä¿®æ­£å¤„æ·»åŠ ã€ï¼Ÿã€‘æ ‡æ³¨ã€‚
3. æ·»åŠ æ–‡è¨€æ–‡é€‚ç”¨çš„æ ‡ç‚¹ç¬¦å·ï¼ˆå¥å·ã€é€—å·ã€é¡¿å·ã€é—®å·ï¼‰ã€‚
4. åˆ é™¤æ‰€æœ‰æœºæ„å…ƒæ•°æ®ï¼ˆå¦‚é¡µç ã€å›¾ä¹¦é¦†æ ‡è®°ã€[ç©ºé¡µ]ç­‰ï¼‰ã€‚
5. å°†æ–‡æœ¬æ•´ç†æˆè¿è´¯æ®µè½ï¼Œä½†**ä¿æŒåŸæ–‡çš„ç« èŠ‚å±‚æ¬¡å’Œè‡ªç„¶æ¢è¡Œ**ã€‚
6. å¯¹äºæ— æ³•ç¡®å®šçš„å­—ï¼Œç”¨ã€ï¼Ÿã€‘æ ‡æ³¨ã€‚
7. **ç‰¹åˆ«æ³¨æ„ï¼šå¦‚æœå‘ç°é‡å¤å‡ºç°çš„æ–‡æœ¬å—ï¼ˆOCRè¯†åˆ«å¾ªç¯é”™è¯¯ï¼‰ï¼Œè¯·åˆ é™¤è¿™äº›é‡å¤éƒ¨åˆ†**ã€‚
8. **å¦‚æœæ–‡æœ¬æ˜æ˜¾ä¸å®Œæ•´æˆ–è¢«æˆªæ–­ï¼Œä¿æŒåŸæ ·å³å¯**ã€‚
9. ã€æ–°å¢ã€‘**å¯¹äºå¤æ±‰è¯­ä¸­çš„å¼‚ä½“å­—ã€é€šå‡å­—ã€é¿è®³å­—ç­‰ï¼Œå³ä½¿çœ‹èµ·æ¥ä¸å¸¸è§ï¼Œä¹Ÿå¿…é¡»ä¿ç•™åŸå­—**ã€‚

é‡è¦æé†’ï¼šå½“ä¸ç¡®å®šæ˜¯å¦åº”è¯¥ä¿®æ­£æ—¶ï¼Œä¸€å¾‹é€‰æ‹©**ä¿ç•™åŸå­—**ã€‚"""

        if has_marked_context:
            context_instruction = """

**é‡è¦è¯´æ˜ - å…³äºä¸Šæ–‡å‚è€ƒ**ï¼š
- æ–‡æœ¬å¼€å¤´æœ‰ [[ä¸Šæ–‡å‚è€ƒ...]] æ‹¬èµ·çš„éƒ¨åˆ†ï¼Œè¿™æ˜¯å‰ä¸€æ®µçš„ç»“å°¾ï¼Œå·²ç»å¤„ç†è¿‡
- [[...]] ä¸­çš„å†…å®¹ä»…ä¾›ä½ ç†è§£ä¸Šä¸‹æ–‡ï¼Œå¸®åŠ©ä½ æ­£ç¡®ç†è§£æ¥ä¸‹æ¥çš„æ–‡æœ¬
- **ç»å¯¹ä¸è¦åœ¨è¾“å‡ºä¸­åŒ…å«æˆ–é‡å¤ [[...]] ä¸­çš„ä»»ä½•å†…å®¹**
- åªå¤„ç†å¹¶è¾“å‡º [[...]] ä¹‹åçš„æ–‡æœ¬
- å¦‚æœ [[...]] åçš„æ–‡æœ¬å¼€å¤´æ˜¯ä¸å®Œæ•´çš„å¥å­ï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡ç†è§£å…¶å«ä¹‰ï¼Œä½†ä»ç„¶ä¸è¦è¾“å‡ºä¸Šæ–‡å‚è€ƒçš„å†…å®¹
- ç›´æ¥ä» [[...]] åé¢çš„ç¬¬ä¸€ä¸ªå­—å¼€å§‹è¾“å‡ºä½ çš„å¤„ç†ç»“æœ"""

            prompt = f"""è¯·å°†ä»¥ä¸‹OCRæ–‡æœ¬æ•´ç†æˆè¿è´¯çš„æ–‡è¨€æ–‡æ–‡çŒ®ã€‚é€™æ˜¯æ˜æ¸…æ™‚æœŸçš„æ–‡ç»ã€‚

è¦æ±‚ï¼š
{base_requirements}{loop_warning}{context_instruction}

{f"ä¸Šä¸‹æ–‡æç¤ºï¼š{context}" if context else ""}

å¾…å¤„ç†æ–‡æœ¬ï¼š
{chunk_text}

è¯·ç›´æ¥è¿”å›æ•´ç†åçš„æ–‡æœ¬ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è¯´æ˜ã€åˆ—è¡¨æˆ–æ ‡é¢˜ï¼Œç«‹å³å¼€å§‹æ­£æ–‡ï¼š"""
        else:
            prompt = f"""è¯·å°†ä»¥ä¸‹OCRæ–‡æœ¬æ•´ç†æˆè¿è´¯çš„æ–‡è¨€æ–‡æ–‡çŒ®ã€‚é€™æ˜¯æ˜æ¸…æ™‚æœŸçš„æ–‡ç»ã€‚

è¦æ±‚ï¼š
{base_requirements}{loop_warning}

{f"ä¸Šä¸‹æ–‡æç¤ºï¼š{context}" if context else ""}

OCRæ–‡æœ¬ï¼š
{chunk_text}

è¯·ç›´æ¥è¿”å›æ•´ç†åçš„æ–‡æœ¬ï¼Œä¸è¦åŒ…å«ä»»ä½•é¢å¤–çš„è¯´æ˜ã€åˆ—è¡¨æˆ–æ ‡é¢˜ï¼Œç«‹å³å¼€å§‹æ­£æ–‡ï¼š"""

        return prompt

    def _execute_with_adaptive_concurrency(self, tasks: List[dict],
                                          max_workers: int) -> List[dict]:
        """Execute tasks with adaptive concurrency control."""
        results = []
        task_queue = tasks.copy()

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_task = {}

            # Initial submission
            initial_batch = min(max_workers * 2, len(task_queue))
            for _ in range(initial_batch):
                if task_queue:
                    task = task_queue.pop(0)
                    future = executor.submit(self._process_single_task, task)
                    future_to_task[future] = task

            # Process completed futures
            while future_to_task:
                done, _ = concurrent.futures.wait(
                    future_to_task.keys(),
                    timeout=1.0,
                    return_when=concurrent.futures.FIRST_COMPLETED
                )

                for future in done:
                    task = future_to_task.pop(future)

                    try:
                        result = future.result(timeout=0)
                        results.append(result)

                        # Update performance metrics
                        if 'processing_time' in result:
                            with self.lock:
                                self.response_times.append(result['processing_time'])
                                if len(self.response_times) > 100:
                                    self.response_times.pop(0)

                        # Submit new task if queue not empty
                        if task_queue:
                            next_task = task_queue.pop(0)
                            new_future = executor.submit(self._process_single_task, next_task)
                            future_to_task[new_future] = next_task

                    except Exception as e:
                        print(f"   âœ— Task {task['index']+1} failed: {e}")
                        results.append({
                            'index': task['index'],
                            'result': task['chunk_text'],
                            'status': 'failed'
                        })

        return results

    def _process_single_task(self, task: dict) -> dict:
        """Process a single task with rate limiting."""
        task_idx = task['index'] + 1
        start_time = time.time()

        # Apply rate limiting if needed
        self._apply_rate_limits(task['estimated_tokens'])

        for attempt in range(self.config.retry_attempts):
            try:
                client, model = self._get_cleanup_client_and_model()

                response = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": task['prompt']}],
                    timeout=self.config.api_timeout,
                    max_tokens=2048
                )

                processing_time = time.time() - start_time
                result = self.sanitize_output(response.choices[0].message.content)

                # Update rate limiting counters
                self._update_counters(task['estimated_tokens'])

                # Print progress
                with self.lock:
                    self.success_count += 1
                    print(f"   âœ“ Chunk {task_idx} ({processing_time:.1f}s)", flush=True)

                return {
                    'index': task['index'],
                    'result': result,
                    'status': 'success',
                    'processing_time': processing_time
                }

            except Exception as e:
                if attempt < self.config.retry_attempts - 1:
                    wait_time = self.config.retry_delay * (2 ** attempt)
                    print(f"   âš ï¸ Chunk {task_idx} retry {attempt+1}: {e}")
                    time.sleep(wait_time)
                else:
                    with self.lock:
                        self.failure_count += 1
                        print(f"   âœ— Chunk {task_idx} failed after retries")

                    return {
                        'index': task['index'],
                        'result': task['chunk_text'],
                        'status': 'failed',
                        'error': str(e)
                    }

    def _apply_rate_limits(self, estimated_tokens: int):
        """Apply rate limits (Kimi only)."""
        if self.service != "kimi":
            return

        current_time = time.time()
        window_elapsed = current_time - self.window_start

        if window_elapsed > 60:
            with self.lock:
                self.token_counter = 0
                self.request_counter = 0
                self.window_start = current_time
            return

        with self.lock:
            projected_tokens = self.token_counter + estimated_tokens

            if projected_tokens > self.limits["max_tokens_per_minute"]:
                tokens_over = projected_tokens - self.limits["max_tokens_per_minute"]
                wait_time = (tokens_over / self.limits["max_tokens_per_minute"]) * 60
                wait_time = min(wait_time, 60 - window_elapsed)

                if wait_time > 0.1:
                    print(f"   â³ TPM limit: waiting {wait_time:.1f}s")
                    time.sleep(wait_time)

                self.token_counter = 0
                self.window_start = time.time()

    def _update_counters(self, tokens_used: int):
        """Update rate limiting counters."""
        if self.service != "kimi":
            return

        with self.lock:
            self.token_counter += tokens_used
            self.request_counter += 1

    def _combine_results(self, results: List[dict], total_chunks: int) -> str:
        """Combine and deduplicate results."""
        sorted_results = sorted(results, key=lambda x: x['index'])
        cleaned_chunks = [r['result'] for r in sorted_results]

        print(f"\nğŸ”— Combining {len(cleaned_chunks)} results with deduplication...")

        if len(cleaned_chunks) <= 1:
            return cleaned_chunks[0] if cleaned_chunks else ""

        deduped_chunks = [cleaned_chunks[0]]

        for i in range(1, len(cleaned_chunks)):
            prev_chunk = deduped_chunks[-1]
            curr_chunk = cleaned_chunks[i]

            duplicate_len = self.detect_overlap_duplicate(
                prev_tail=prev_chunk[-800:],
                curr_head=curr_chunk[:800],
                threshold=0.85,
                min_length=200
            )

            if duplicate_len > 0:
                print(f"   Removed {duplicate_len}-char duplicate between chunks {i}â†’{i+1}")
                deduped_chunks.append(curr_chunk[duplicate_len:])
            else:
                deduped_chunks.append(curr_chunk)

        combined = '\n\n'.join(deduped_chunks)

        failed_count = sum(1 for r in results if r['status'] == 'failed')
        if failed_count > 0:
            print(f"âš ï¸  Warning: {failed_count} chunks failed and used original text")

        return combined

    def _print_performance_summary(self, elapsed: float,
                                  total_chunks: int, output_chars: int):
        """Print performance summary."""
        chars_per_sec = output_chars / elapsed if elapsed > 0 else 0

        print(f"\n{'='*60}")
        print(f"ğŸš€ PARALLEL PROCESSING COMPLETE")
        print(f"{'='*60}")
        print(f"   Service:        {self.service.upper()}")
        print(f"   Total time:     {elapsed:.1f}s")
        print(f"   Chunks:         {total_chunks}")
        print(f"   Output length:  {output_chars} chars")
        print(f"   Throughput:     {chars_per_sec:.0f} chars/sec")
        print(f"   Success rate:   {self.success_count}/{total_chunks} chunks")

        if self.response_times:
            avg_time = sum(self.response_times) / len(self.response_times)
            print(f"   Avg response:   {avg_time:.1f}s")

        print(f"{'='*60}")

# ============================================================================
# DOCUMENT PROCESSOR
# ============================================================================

class DocumentProcessor:
    """Main document processing orchestrator"""

    def __init__(self, config: ProcessingConfig):
        self.config = config
        self.clients = APIClients(cleanup_model=config.cleanup_model)
        self.ocr_engine = OCREngine(self.clients, config)
        self.text_cleaner = HighConcurrencyTextCleaner(self.clients, config)
        self.zotero = ZoteroExporter()

    def process_image(self, image_path: str, context: str = "",
                     output_dir: str = "./processed",
                     review_mode: bool = False) -> Optional[Path]:
        """Process a single image file"""
        print(f"\n{'='*60}", flush=True)
        print(f"Processing image: {image_path}", flush=True)
        print(f"{'='*60}", flush=True)

        # OCR
        result = self.ocr_engine.process_image(image_path, 1, 1)
        if not result:
            print("âŒ OCR failed", flush=True)
            return None

        raw_text, has_loops = result

        # Save raw OCR
        os.makedirs(output_dir, exist_ok=True)
        doc_name = Path(image_path).stem

        # Clean text
        print("\n=== Text Cleaning Stage ===", flush=True)
        cleaned_text = self.text_cleaner.process_text_in_chunks(raw_text, context, has_loops)

        # Save markdown (ALWAYS)
        pages_with_loops = [1] if has_loops else []
        output_path = self._create_consolidated_note(
            doc_name, cleaned_text, pages_with_loops,
            output_dir, context, 1
        )
        print(f"âœ“ Markdown saved: {output_path}", flush=True)

        # Optional: Generate quality review report
        if review_mode:
            review_report = self.text_cleaner.generate_review_report(cleaned_text, context)
            review_path = Path(output_dir) / f"{doc_name}_review.txt"
            with open(review_path, 'w', encoding='utf-8') as f:
                f.write(f"# è´¨é‡å®¡æŸ¥æŠ¥å‘Š\n\n")
                f.write(f"æ–‡æ¡£: {doc_name}\n")
                f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"æ–‡æœ¬é•¿åº¦: {len(cleaned_text)} å­—ç¬¦\n\n")
                f.write("="*60 + "\n\n")
                f.write(review_report)
            print(f"âœ“ Review report saved: {review_path}", flush=True)

        print(f"\nâœ“ Processing complete: {output_path}", flush=True)
        return output_path

    def process_pdf(self, pdf_path: str, context: str = "",
                   output_dir: str = "./processed",
                   review_mode: bool = False, max_pages: Optional[int] = None,
                   start_page: int = 1,
                   export_zotero: bool = False,
                   zotero_title: Optional[str] = None,
                   zotero_collection: Optional[str] = None) -> Optional[Path]:
        """Process a PDF file"""
        print(f"\n{'='*60}", flush=True)
        print(f"Processing PDF: {pdf_path}", flush=True)
        print(f"Output directory: {output_dir}", flush=True)
        print(f"{'='*60}", flush=True)

        if not PDF_SUPPORT:
            print("âŒ PDF support not available", flush=True)
            return None

        try:
            # Get page info
            info = pdfinfo_from_path(pdf_path)
            total_pages = info["Pages"]
            end_page = min(start_page + max_pages - 1, total_pages) if max_pages else total_pages
            page_count = end_page - start_page + 1

            print(f"Total PDF pages: {total_pages}", flush=True)
            print(f"Processing pages {start_page} to {end_page} ({page_count} pages)", flush=True)
            print(f"DPI: {self.config.ocr_dpi}, Model: {self.config.qwen_model}", flush=True)

            # Create output directory
            os.makedirs(output_dir, exist_ok=True)
            doc_name = Path(pdf_path).stem

            # OCR Stage
            print("\n" + "="*60, flush=True)
            print("STAGE 1: OCR EXTRACTION", flush=True)
            print("="*60, flush=True)

            all_ocr_texts = []
            pages_with_loops = []
            start_time = time.time()

            current_page_index = 0
            for page_num in range(start_page, end_page + 1):
                current_page_index += 1

                # Convert single page
                images = convert_from_path(
                    pdf_path,
                    dpi=self.config.ocr_dpi,
                    first_page=page_num,
                    last_page=page_num
                )

                if not images:
                    print(f"âš ï¸  Skipping page {page_num} - conversion failed", flush=True)
                    continue

                # Save as temporary image
                temp_image = tempfile.NamedTemporaryFile(suffix='.png', delete=False)
                images[0].save(temp_image.name, 'PNG')

                try:
                    # Process with OCR (pass current index for accurate progress)
                    result = self.ocr_engine.process_image(temp_image.name, page_num, page_count, current_page_index)
                    if result:
                        text, has_loops = result
                        all_ocr_texts.append(text)
                        if has_loops:
                            pages_with_loops.append(page_num)
                finally:
                    # Clean up temp image
                    if os.path.exists(temp_image.name):
                        os.unlink(temp_image.name)

            ocr_time = time.time() - start_time

            if not all_ocr_texts:
                print("âŒ No text extracted from any pages", flush=True)
                return None

            # Combine OCR texts
            combined_ocr = '\n\n'.join(all_ocr_texts)
            print(f"\nâœ“ OCR complete: {len(combined_ocr)} chars from {len(all_ocr_texts)} pages", flush=True)
            print(f"   Time: {ocr_time:.1f}s", flush=True)
            print(f"   Pages with loops: {len(pages_with_loops)}", flush=True)

            # Save OCR backup
            backup_data = {
                "document": doc_name,
                "source": str(pdf_path),
                "pages_processed": page_count,
                "pages_with_loops": pages_with_loops,
                "raw_ocr_text": combined_ocr,
                "context": context,
                "timestamp": datetime.now().isoformat(),
                "config": {
                    "model": self.config.qwen_model,
                    "dpi": self.config.ocr_dpi,
                }
            }

            backup_path = Path(output_dir) / f"{doc_name}_raw_ocr_latest.json"
            with open(backup_path, 'w', encoding='utf-8') as f:
                json.dump(backup_data, f, ensure_ascii=False, indent=2)
            print(f"   Backup saved: {backup_path}", flush=True)

            # Cleaning Stage
            print("\n" + "="*60, flush=True)
            print("STAGE 2: TEXT CLEANING", flush=True)
            print("="*60, flush=True)

            has_loops = len(pages_with_loops) > 0
            cleaned_text = self.text_cleaner.process_text_in_chunks(
                combined_ocr, context, has_loops
            )

            # Save markdown after Stage 2 (ALWAYS - prevents data loss)
            print("\n" + "="*60, flush=True)
            print("SAVING OUTPUT", flush=True)
            print("="*60, flush=True)

            output_path = self._create_consolidated_note(
                doc_name, cleaned_text, pages_with_loops,
                output_dir, context, page_count
            )
            print(f"âœ“ Markdown saved: {output_path}", flush=True)

            # Optional: Generate quality review report
            if review_mode:
                review_report = self.text_cleaner.generate_review_report(cleaned_text, context)
                review_path = Path(output_dir) / f"{doc_name}_review.txt"
                with open(review_path, 'w', encoding='utf-8') as f:
                    f.write(f"# è´¨é‡å®¡æŸ¥æŠ¥å‘Š\n\n")
                    f.write(f"æ–‡æ¡£: {doc_name}\n")
                    f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"æ–‡æœ¬é•¿åº¦: {len(cleaned_text)} å­—ç¬¦\n\n")
                    f.write("="*60 + "\n\n")
                    f.write(review_report)
                print(f"âœ“ Review report saved: {review_path}", flush=True)

            total_time = time.time() - start_time
            print(f"\n{'='*60}", flush=True)
            print(f"âœ“ Processing complete in {total_time:.1f}s", flush=True)
            print(f"âœ“ Output: {output_path}", flush=True)
            print(f"{'='*60}", flush=True)

            # Export to Zotero if requested
            if export_zotero:
                print(f"\nğŸ“š Exporting to Zotero...", flush=True)
                collection_key = None
                if zotero_collection:
                    collection_key = self.zotero.get_collection_key(zotero_collection)
                
                self.zotero.export_to_zotero(
                    output_path,
                    pdf_path,
                    self.config.qwen_model,
                    page_count,
                    title=zotero_title,
                    collection_key=collection_key
                )

            return output_path

        except Exception as e:
            print(f"âŒ Error processing PDF: {e}", flush=True)
            import traceback
            traceback.print_exc()
            return None

    def resume_from_backup(self, backup_path: str, context: str = "",
                          output_dir: str = "./processed",
                          review_mode: bool = False,
                          export_zotero: bool = False,
                          zotero_title: Optional[str] = None,
                          zotero_collection: Optional[str] = None):
        """Resume processing from a raw OCR backup file"""
        print(f"\n{'='*60}", flush=True)
        print(f"Resuming from backup: {backup_path}", flush=True)
        print(f"{'='*60}", flush=True)

        try:
            with open(backup_path, 'r', encoding='utf-8') as f:
                backup_data = json.load(f)

            doc_name = backup_data["document"]
            combined_ocr = backup_data["raw_ocr_text"]
            pages_with_loops = backup_data.get("pages_with_loops", [])
            page_count = backup_data.get("pages_processed", 0)
            saved_context = backup_data.get("context", "")

            # Use provided context or fall back to saved context
            final_context = context if context else saved_context

            print(f"Document: {doc_name}", flush=True)
            print(f"Text length: {len(combined_ocr)} chars", flush=True)
            print(f"Pages with loops: {len(pages_with_loops)}", flush=True)

            # Cleaning Stage
            print("\n" + "="*60, flush=True)
            print("STAGE 2: TEXT CLEANING (RESUMED)", flush=True)
            print("="*60, flush=True)
            print("", flush=True)  # Empty line for readability

            has_loops = len(pages_with_loops) > 0
            print("Starting text chunking and cleaning...", flush=True)
            cleaned_text = self.text_cleaner.process_text_in_chunks(
                combined_ocr, final_context, has_loops
            )

            # Save markdown (ALWAYS)
            print("\n" + "="*60, flush=True)
            print("SAVING OUTPUT", flush=True)
            print("="*60, flush=True)

            output_path = self._create_consolidated_note(
                doc_name, cleaned_text, pages_with_loops,
                output_dir, final_context, page_count
            )
            print(f"âœ“ Markdown saved: {output_path}", flush=True)

            # Optional: Generate quality review report
            if review_mode:
                review_report = self.text_cleaner.generate_review_report(cleaned_text, final_context)
                review_path = Path(output_dir) / f"{doc_name}_review.txt"
                with open(review_path, 'w', encoding='utf-8') as f:
                    f.write(f"# è´¨é‡å®¡æŸ¥æŠ¥å‘Š\n\n")
                    f.write(f"æ–‡æ¡£: {doc_name}\n")
                    f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"æ–‡æœ¬é•¿åº¦: {len(cleaned_text)} å­—ç¬¦\n\n")
                    f.write("="*60 + "\n\n")
                    f.write(review_report)
                print(f"âœ“ Review report saved: {review_path}", flush=True)

            print(f"\nâœ“ Resume processing complete: {output_path}", flush=True)

            # Export to Zotero if requested
            if export_zotero:
                print(f"\nğŸ“š Exporting to Zotero...", flush=True)
                collection_key = None
                if zotero_collection:
                    collection_key = self.zotero.get_collection_key(zotero_collection)
                
                source_file = backup_data.get("source", doc_name)
                self.zotero.export_to_zotero(
                    output_path,
                    source_file,
                    self.config.qwen_model,
                    page_count,
                    title=zotero_title,
                    collection_key=collection_key
                )

            return output_path

        except Exception as e:
            print(f"âŒ Error resuming from backup: {e}", flush=True)
            import traceback
            traceback.print_exc()
            return None

    def _create_consolidated_note(self, document_name: str, full_cleaned_text: str,
                                pages_with_loops: List[int],
                                output_dir: str, context: str, page_count: int) -> Path:
        """Create consolidated markdown note"""
        collection = Path(output_dir).name if output_dir != "./processed" else Path(document_name).stem

        # Generate quality section if there were problematic pages
        quality_section = ""
        if pages_with_loops:
            quality_section = f"""
## âš ï¸ OCRè´¨é‡æç¤º

æ£€æµ‹åˆ° {len(pages_with_loops)} é¡µå¯èƒ½åŒ…å«OCRå¾ªç¯é”™è¯¯ï¼Œå·²æ ‡è®°ä¾›Kimiæ¸…ç†ï¼š

**éœ€è¦æ£€æŸ¥çš„é¡µé¢**: {', '.join(map(str, pages_with_loops[:20]))}
{f"...åŠå…¶ä»– {len(pages_with_loops)-20} é¡µ" if len(pages_with_loops) > 20 else ""}

è¿™äº›é¡µé¢çš„é‡å¤å†…å®¹åº”è¯¥å·²è¢«è‡ªåŠ¨æ¸…ç†ï¼Œä½†å»ºè®®äººå·¥å¤æ ¸ã€‚

---
"""

        # Build the document
        note_content = f"""---
type: primary-source
source: {document_name}
collection: {collection}
date-processed: {datetime.now().strftime('%Y-%m-%d')}
total-pages: {page_count}
flagged-pages: {len(pages_with_loops)}
status: {"needs-review" if pages_with_loops else "clean"}
ocr-engine: Qwen-VL
tags:
- primary-source
- {collection}
- Ming-Qing
---

# {document_name}

> [!info] Document Metadata
> - **Collection**: {collection}
> - **Total Pages**: {page_count}
> - **Flagged Pages**: {len(pages_with_loops)}/{page_count}
> - **Processing Date**: {datetime.now().strftime('%Y-%m-%d %H:%M')}
> - **OCR Engine**: Qwen-VL ({self.config.qwen_model})
> - **Context**: {context if context else "None"}
> - **Status**: {"ğŸŸ¡ Needs Review" if pages_with_loops else "ğŸŸ¢ Clean"}

---

{quality_section}

## å…¨æ–‡æ•´ç†

{full_cleaned_text}
"""

        # Add research notes section
        note_content += f"""
---

## ç ”ç©¶ç¬”è®°

### å…³é”®æœ¯è¯­
-

### å†å²è¯­å¢ƒ
-

### ç›¸å…³æ–‡çŒ®
-

---

## å¤„ç†å†å²
- {datetime.now().strftime('%Y-%m-%d')}: OCR (Qwen-VL) and initial processing
- Pages processed: {page_count}
- Model used: {self.config.qwen_model}
- Pages with loops: {len(pages_with_loops)}
"""

        # Save file
        output_path = Path(output_dir) / f"{document_name}.md"
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(note_content)

        print(f"\nâœ“ Created consolidated note: {output_path}", flush=True)
        return output_path

# ============================================================================
# COMMAND LINE INTERFACE
# ============================================================================

def create_argument_parser() -> argparse.ArgumentParser:
    """Create command line argument parser"""
    parser = argparse.ArgumentParser(
        description='Process classical Chinese documents with AI',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Process single image
  %(prog)s scan.jpg --output ./processed

  # Process PDF with context
  %(prog)s document.pdf --context "æ˜ä»£æ–‡é›†ï¼Œç«–æ’æ— æ ‡ç‚¹" --output ~/Obsidian/Primary-Sources/

  # Generate quality review report (optional)
  %(prog)s document.pdf --review --output ./processed

  # Use alternative OCR model
  %(prog)s document.pdf --model qwen-vl-max --output ./processed

  # Use DeepSeek for text cleanup (cheaper alternative to Kimi)
  %(prog)s document.pdf --cleanup-model deepseek --output ./processed

  # Process pages 50-100 only
  %(prog)s document.pdf --start-page 50 --max-pages 51 --output ./processed

  # Process from page 100 to end
  %(prog)s document.pdf --start-page 100 --output ./processed

  # Limit pages for testing
  %(prog)s document.pdf --max-pages 5 --output ./processed

  # Resume from OCR backup
  %(prog)s --resume-from ./processed/document_raw_ocr_latest.json --output ./processed

  # Export to Zotero
  %(prog)s document.pdf --zotero --zotero-collection "Primary Sources" --output ./processed

  # Batch process directory
  %(prog)s ./scans/ --batch --context "æ³‰å·åºœå¿—" --output ~/Obsidian/Primary-Sources

  # Force sequential processing (for debugging)
  %(prog)s document.pdf --sequential --output ./processed

  # Limit maximum concurrent requests
  %(prog)s document.pdf --max-concurrent 20 --output ./processed
"""
    )

    parser.add_argument('input', nargs='?', help='Image file, PDF, or directory (not required with --resume-from)')
    parser.add_argument('--context', default='', help='Contextual information')
    parser.add_argument('--output', default='./processed', help='Output directory')
    parser.add_argument('--batch', action='store_true', help='Process all files in directory')
    parser.add_argument('--dpi', type=int, default=300, help='DPI for PDF conversion (default: 300 for optimal OCR)')
    parser.add_argument('--model', default='qwen3-vl-plus', choices=['qwen3-vl-plus', 'qwen-vl-max'], help='Qwen OCR model')
    parser.add_argument('--cleanup-model', default='deepseek', choices=['kimi', 'deepseek'], help='Text cleanup model (default: deepseek)')
    parser.add_argument('--review', action='store_true', help='Generate quality review report after processing')
    parser.add_argument('--max-pages', type=int, help='Limit processing to first N pages')
    parser.add_argument('--start-page', type=int, default=1, help='Start processing from page N (default: 1)')
    parser.add_argument('--end-page', type=int, help='Last page to process (alternative to max-pages)')
    parser.add_argument('--resume-from', help='Resume from raw OCR JSON file')
    parser.add_argument('--sequential', action='store_true', help='Force sequential processing (disables parallel)')
    parser.add_argument('--max-concurrent', type=int, default=None, help='Maximum concurrent requests (default: auto-detected)')
    parser.add_argument('--zotero', action='store_true', help='Export to Zotero after processing')
    parser.add_argument('--zotero-title', help='Custom title for Zotero item (defaults to filename)')
    parser.add_argument('--zotero-collection', help='Zotero collection name to file the item in')

    return parser

def main():
    """Main entry point"""
    parser = create_argument_parser()
    args = parser.parse_args()

    # Validate API keys
    if not APIClients.validate_keys():
        sys.exit(1)

    # Initialize configuration
    config = ProcessingConfig()
    config.update_from_args(args)

    # Define start and end pages
    if args.end_page:
        if args.start_page:
            # If both start and end specified, calculate max_pages
            args.max_pages = args.end_page - args.start_page + 1
        else:
            # If only end specified, process from page 1 to end
            args.max_pages = args.end_page

    # Initialize processor
    processor = DocumentProcessor(config)

    # Handle sequential mode by replacing text cleaner
    if args.sequential:
        print("âš ï¸  Sequential mode forced (--sequential flag)", flush=True)
        processor.text_cleaner = TextCleaner(processor.clients, config)
    # Apply max_concurrent override if specified
    elif args.max_concurrent:
        processor.text_cleaner.limits["max_concurrent"] = args.max_concurrent
        print(f"âš™ï¸  Concurrency limit set to {args.max_concurrent} workers", flush=True)

    # Handle resume case
    if args.resume_from:
        if not Path(args.resume_from).exists():
            print(f"âŒ Error: Backup file {args.resume_from} not found", flush=True)
            sys.exit(1)
        processor.resume_from_backup(
            args.resume_from, args.context, args.output, args.review,
            export_zotero=args.zotero,
            zotero_title=args.zotero_title,
            zotero_collection=args.zotero_collection
        )
        return

    # Validate input
    if not args.input:
        print("âŒ Error: Input file or directory required when not using --resume-from", flush=True)
        sys.exit(1)

    # Batch processing
    if args.batch:
        input_path = Path(args.input)
        if not input_path.is_dir():
            print(f"âŒ Error: {args.input} is not a directory", flush=True)
            sys.exit(1)

        images = list(input_path.glob('*.jpg')) + list(input_path.glob('*.png')) + list(input_path.glob('*.jpeg'))
        pdfs = list(input_path.glob('*.pdf'))

        if not images and not pdfs:
            print(f"âŒ No images or PDFs found in {args.input}", flush=True)
            sys.exit(1)

        print(f"Found {len(images)} image(s) and {len(pdfs)} PDF(s) to process\n", flush=True)
        print("=" * 60, flush=True)

        successful = 0
        failed = 0

        for img in images:
            result = processor.process_image(str(img), args.context, args.output, args.review)
            if result:
                successful += 1
            else:
                failed += 1

        for pdf in pdfs:
            result = processor.process_pdf(
                str(pdf), args.context, args.output, args.review, 
                args.max_pages, args.start_page,
                export_zotero=args.zotero,
                zotero_title=args.zotero_title,
                zotero_collection=args.zotero_collection
            )
            if result:
                successful += 1
            else:
                failed += 1

        print("=" * 60, flush=True)
        print(f"\nğŸ“Š Processing complete:", flush=True)
        print(f"   âœ“ Successful: {successful}", flush=True)
        print(f"   âœ— Failed: {failed}", flush=True)

    else:
        # Single file processing
        if not Path(args.input).exists():
            print(f"âŒ Error: File {args.input} not found", flush=True)
            sys.exit(1)

        file_ext = Path(args.input).suffix.lower()
        if file_ext == '.pdf':
            if not PDF_SUPPORT:
                print("âŒ Error: PDF support not available", flush=True)
                print("   Install with: pip install pdf2image", flush=True)
                sys.exit(1)
            processor.process_pdf(
                args.input, args.context, args.output, args.review, 
                args.max_pages, args.start_page,
                export_zotero=args.zotero,
                zotero_title=args.zotero_title,
                zotero_collection=args.zotero_collection
            )
        else:
            processor.process_image(args.input, args.context, args.output, args.review)

        print("\nâœ“ Processing complete!", flush=True)

if __name__ == "__main__":
    main()
